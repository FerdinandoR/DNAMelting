{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA Melting\n",
    "An investigation of the efficacy of several machine learning models to predict DNA melting temperature. \n",
    "\n",
    "For all of intents and purposes of this notebook, we only need to know that the DNA melting temperature is a number that depends on two other numbers (the duplex and salt concentrations) and on a string (representing the DNA sequence) composed of A, C, G, T. Similar strings tend to have a similar melting temperature.\n",
    "\n",
    "We are going to use the temperatures as predicted by the state-of-the-art model, the SantaLucia model, as ground truth. For more information on the model, you can see the references in `SantaLucia.py`, which encodes a way to compute the melting temperatures. We use that file in `make_melting_database.py` to produce the database to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import a few packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import SantaLucia as sl\n",
    "import make_melting_database\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "bases = ['A', 'C', 'G', 'T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for the first time that the notebook is run, or when we want to add/change the data stored in the melting temperature database, we use `write_database` from `make_melting_database.py` to generate the melting temperature database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_database = False\n",
    "if write_database:\n",
    "    start_n_bases = 6\n",
    "    end_n_bases = 9\n",
    "    duplex_umol_conc = [1,2]\n",
    "    salt_mol_conc = [0.1, 0.2, 0.5]\n",
    "    make_melting_database.write_database(start_n_bases, end_n_bases, duplex_umol_conc, salt_mol_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melting_database\\\\6meres.csv', 'melting_database\\\\7meres.csv', 'melting_database\\\\8meres.csv', 'melting_database\\\\9meres.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0</th>\n",
       "      <th>base_1</th>\n",
       "      <th>base_2</th>\n",
       "      <th>base_3</th>\n",
       "      <th>base_4</th>\n",
       "      <th>base_5</th>\n",
       "      <th>base_6</th>\n",
       "      <th>base_7</th>\n",
       "      <th>base_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               seq  s_mol  c_umol        T_m base_0 base_1 base_2 base_3  \\\n",
       "0           AAAAAA    0.1       1 -20.495855      A      A      A      A   \n",
       "1           AAAAAA    0.2       1 -17.536560      A      A      A      A   \n",
       "2           AAAAAA    0.5       1 -13.516521      A      A      A      A   \n",
       "3           AAAAAA    0.1       2 -17.837395      A      A      A      A   \n",
       "4           AAAAAA    0.2       2 -14.815124      A      A      A      A   \n",
       "...            ...    ...     ...        ...    ...    ...    ...    ...   \n",
       "2088955  GGGGGGGGG    0.2       1  47.545073      G      G      G      G   \n",
       "2088956  GGGGGGGGG    0.5       1  52.512866      G      G      G      G   \n",
       "2088957  GGGGGGGGG    0.1       2  46.070145      G      G      G      G   \n",
       "2088958  GGGGGGGGG    0.2       2  49.779454      G      G      G      G   \n",
       "2088959  GGGGGGGGG    0.5       2  54.817256      G      G      G      G   \n",
       "\n",
       "        base_4 base_5 base_6 base_7 base_8  \n",
       "0            A      A      0      0      0  \n",
       "1            A      A      0      0      0  \n",
       "2            A      A      0      0      0  \n",
       "3            A      A      0      0      0  \n",
       "4            A      A      0      0      0  \n",
       "...        ...    ...    ...    ...    ...  \n",
       "2088955      G      G      G      G      G  \n",
       "2088956      G      G      G      G      G  \n",
       "2088957      G      G      G      G      G  \n",
       "2088958      G      G      G      G      G  \n",
       "2088959      G      G      G      G      G  \n",
       "\n",
       "[2088960 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather the data from the various files and pool them into one dataframe\n",
    "database_dir = 'melting_database'\n",
    "datasets = os.listdir(database_dir)\n",
    "\n",
    "all_files = [os.path.join(database_dir, data) for data in datasets]\n",
    "print(all_files)\n",
    "df = pd.concat((pd.read_csv(f, index_col=0) for f in all_files), ignore_index=True)\n",
    "\n",
    "df\n",
    "#dataset = torch.utils.data.TensorDataset(x, T_m)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-hot encoded bases fed to a fully connected network\n",
    "\n",
    "The simplest model we will try is a fully connected network attached connected to one-hot encoded nucleotides. First of all, we perform the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0_0</th>\n",
       "      <th>base_0_A</th>\n",
       "      <th>base_0_C</th>\n",
       "      <th>base_0_G</th>\n",
       "      <th>base_0_T</th>\n",
       "      <th>base_1_0</th>\n",
       "      <th>base_1_A</th>\n",
       "      <th>...</th>\n",
       "      <th>base_7_0</th>\n",
       "      <th>base_7_A</th>\n",
       "      <th>base_7_C</th>\n",
       "      <th>base_7_G</th>\n",
       "      <th>base_7_T</th>\n",
       "      <th>base_8_0</th>\n",
       "      <th>base_8_A</th>\n",
       "      <th>base_8_C</th>\n",
       "      <th>base_8_G</th>\n",
       "      <th>base_8_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         s_mol  c_umol        T_m  base_0_0  base_0_A  base_0_C  base_0_G  \\\n",
       "0          0.1       1 -20.495855         0         1         0         0   \n",
       "1          0.2       1 -17.536560         0         1         0         0   \n",
       "2          0.5       1 -13.516521         0         1         0         0   \n",
       "3          0.1       2 -17.837395         0         1         0         0   \n",
       "4          0.2       2 -14.815124         0         1         0         0   \n",
       "...        ...     ...        ...       ...       ...       ...       ...   \n",
       "2088955    0.2       1  47.545073         0         0         0         1   \n",
       "2088956    0.5       1  52.512866         0         0         0         1   \n",
       "2088957    0.1       2  46.070145         0         0         0         1   \n",
       "2088958    0.2       2  49.779454         0         0         0         1   \n",
       "2088959    0.5       2  54.817256         0         0         0         1   \n",
       "\n",
       "         base_0_T  base_1_0  base_1_A  ...  base_7_0  base_7_A  base_7_C  \\\n",
       "0               0         0         1  ...         1         0         0   \n",
       "1               0         0         1  ...         1         0         0   \n",
       "2               0         0         1  ...         1         0         0   \n",
       "3               0         0         1  ...         1         0         0   \n",
       "4               0         0         1  ...         1         0         0   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "2088955         0         0         0  ...         0         0         0   \n",
       "2088956         0         0         0  ...         0         0         0   \n",
       "2088957         0         0         0  ...         0         0         0   \n",
       "2088958         0         0         0  ...         0         0         0   \n",
       "2088959         0         0         0  ...         0         0         0   \n",
       "\n",
       "         base_7_G  base_7_T  base_8_0  base_8_A  base_8_C  base_8_G  base_8_T  \n",
       "0               0         0         1         0         0         0         0  \n",
       "1               0         0         1         0         0         0         0  \n",
       "2               0         0         1         0         0         0         0  \n",
       "3               0         0         1         0         0         0         0  \n",
       "4               0         0         1         0         0         0         0  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "2088955         1         0         0         0         0         1         0  \n",
       "2088956         1         0         0         0         0         1         0  \n",
       "2088957         1         0         0         0         0         1         0  \n",
       "2088958         1         0         0         0         0         1         0  \n",
       "2088959         1         0         0         0         0         1         0  \n",
       "\n",
       "[2088960 rows x 48 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oh = df.copy()\n",
    "# We add (and then remove) a sequence with all missing bases to ensure that\n",
    "# every position has the same number of dummy columns. Not required for now,\n",
    "# but required for steps 3 and 4\n",
    "df_oh.loc[df.shape[0]] = df.shape[1] * [0]\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    target_col = 'base_' + str(i)\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df[target_col]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # generate the one-hot encoded columns and append them to the dataframe\n",
    "    new_cols = pd.get_dummies(df_oh[target_col], prefix = target_col)\n",
    "    df_oh = pd.concat([df_oh, new_cols], axis=1)\n",
    "    # remove original column from the dataframe\n",
    "    df_oh.drop(target_col, axis=1, inplace=True)\n",
    "    i += 1\n",
    "df_oh.drop('seq', axis=1, inplace=True)\n",
    "df_oh.drop(df_oh.shape[0]-1,inplace=True)\n",
    "df_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data between the training set and the validation set, and load it into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1880064 instances\n",
      "Validation set has 208896 instances\n"
     ]
    }
   ],
   "source": [
    "# Cast the data into a dataset\n",
    "y = torch.Tensor(df.T_m)\n",
    "x = torch.Tensor(df_oh.drop('T_m', axis=1).values)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "\n",
    "train_length = int(len(dataset) * 0.9)\n",
    "valid_length = len(dataset) - train_length\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_length, valid_length])\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_set)} instances')\n",
    "print(f'Validation set has {len(valid_set)} instances')\n",
    "\n",
    "# Create the loaders\n",
    "batch_size = 64\n",
    "training_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "validation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the model. In this part of the notebook we use a simple fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, input_size : int, n_hidden_layers : int, hidden_size : int):\n",
    "        super(FC, self).__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for n in range(self.n_hidden_layers):\n",
    "            x = F.relu(self.hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = FC(x.shape[1], 1, x.shape[1] * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry on defining the loss and the optimizer. We use a standard MSE loss and an Adam optimizer with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop - taken from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop sweeps the data over multiple epochs. Once per epoch, we perform model validation by checking model performance on validation data, and save a copy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.8618826355338096\n",
      "  batch 2000 loss: 0.8778098120987415\n",
      "  batch 3000 loss: 0.8753938832879067\n",
      "  batch 4000 loss: 0.8819746115803718\n",
      "  batch 5000 loss: 0.8798796901106835\n",
      "  batch 6000 loss: 0.8774853567481041\n",
      "  batch 7000 loss: 0.8763367778658867\n",
      "  batch 8000 loss: 0.8780601848363876\n",
      "  batch 9000 loss: 0.8799756965041161\n",
      "  batch 10000 loss: 0.870528419047594\n",
      "  batch 11000 loss: 0.8711632699370384\n",
      "  batch 12000 loss: 0.8766091204881669\n",
      "  batch 13000 loss: 0.879900945365429\n",
      "  batch 14000 loss: 0.8755089908838272\n",
      "  batch 15000 loss: 0.8765862835049629\n",
      "  batch 16000 loss: 0.8800385928750039\n",
      "  batch 17000 loss: 0.8710080534219742\n",
      "  batch 18000 loss: 0.8770153911113739\n",
      "  batch 19000 loss: 0.8770744238495827\n",
      "  batch 20000 loss: 0.8787410282492638\n",
      "  batch 21000 loss: 0.8730585723519325\n",
      "  batch 22000 loss: 0.8789740337133407\n",
      "  batch 23000 loss: 0.8794863363802433\n",
      "  batch 24000 loss: 0.8766420214772225\n",
      "  batch 25000 loss: 0.8778146523237228\n",
      "  batch 26000 loss: 0.877734235227108\n",
      "  batch 27000 loss: 0.8730795223712922\n",
      "  batch 28000 loss: 0.8749839498996734\n",
      "  batch 29000 loss: 0.8672685547173024\n",
      "LOSS train 0.8672685547173024 valid 0.1972825974225998\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.8631651705503464\n",
      "  batch 2000 loss: 0.8666493571400642\n",
      "  batch 3000 loss: 0.8769723136425018\n",
      "  batch 4000 loss: 0.8775643916726112\n",
      "  batch 5000 loss: 0.8741542428135872\n",
      "  batch 6000 loss: 0.881759639441967\n",
      "  batch 7000 loss: 0.8760276717543602\n",
      "  batch 8000 loss: 0.8777535185813904\n",
      "  batch 9000 loss: 0.8704108313024044\n",
      "  batch 10000 loss: 0.8804036135673523\n",
      "  batch 11000 loss: 0.8707992513179779\n",
      "  batch 12000 loss: 0.8706397613286972\n",
      "  batch 13000 loss: 0.8835099852085113\n",
      "  batch 14000 loss: 0.8712776635289192\n",
      "  batch 15000 loss: 0.8715558683872223\n",
      "  batch 16000 loss: 0.8806126787662506\n",
      "  batch 17000 loss: 0.8732439741492272\n",
      "  batch 18000 loss: 0.8769985898137093\n",
      "  batch 19000 loss: 0.8678242188692092\n",
      "  batch 20000 loss: 0.8707221459150314\n",
      "  batch 21000 loss: 0.8725335599184036\n",
      "  batch 22000 loss: 0.8730804690718651\n",
      "  batch 23000 loss: 0.8721691623330117\n",
      "  batch 24000 loss: 0.8659921738505364\n",
      "  batch 25000 loss: 0.8776954155564308\n",
      "  batch 26000 loss: 0.8728897317051888\n",
      "  batch 27000 loss: 0.8727657302618027\n",
      "  batch 28000 loss: 0.8769197223484516\n",
      "  batch 29000 loss: 0.8708108270764351\n",
      "LOSS train 0.8708108270764351 valid 0.22077858448028564\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.8594060983359814\n",
      "  batch 2000 loss: 0.8674439399242401\n",
      "  batch 3000 loss: 0.8682321095466614\n",
      "  batch 4000 loss: 0.8717689206600189\n",
      "  batch 5000 loss: 0.8740086193680763\n",
      "  batch 6000 loss: 0.8748626842498779\n",
      "  batch 7000 loss: 0.8688083094358444\n",
      "  batch 8000 loss: 0.8696223357319832\n",
      "  batch 9000 loss: 0.8762237405776978\n",
      "  batch 10000 loss: 0.8704470657110214\n",
      "  batch 11000 loss: 0.8741071235537529\n",
      "  batch 12000 loss: 0.8761922532916069\n",
      "  batch 13000 loss: 0.8797767027616501\n",
      "  batch 14000 loss: 0.8711574951112271\n",
      "  batch 15000 loss: 0.8713061670064927\n",
      "  batch 16000 loss: 0.8749021791815758\n",
      "  batch 17000 loss: 0.8711786151528359\n",
      "  batch 18000 loss: 0.8686779547929764\n",
      "  batch 19000 loss: 0.879127993941307\n",
      "  batch 20000 loss: 0.8690556135177613\n",
      "  batch 21000 loss: 0.8679368731975555\n",
      "  batch 22000 loss: 0.8733727534413338\n",
      "  batch 23000 loss: 0.8706412061452865\n",
      "  batch 24000 loss: 0.8727801584899425\n",
      "  batch 25000 loss: 0.8755039952397347\n",
      "  batch 26000 loss: 0.8707948772907257\n",
      "  batch 27000 loss: 0.8695013561844825\n",
      "  batch 28000 loss: 0.8742437369227409\n",
      "  batch 29000 loss: 0.8612526379227639\n",
      "LOSS train 0.8612526379227639 valid 0.20712226629257202\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.8673294813632965\n",
      "  batch 2000 loss: 0.8693614110946656\n",
      "  batch 3000 loss: 0.8697211080789566\n",
      "  batch 4000 loss: 0.8692027254104614\n",
      "  batch 5000 loss: 0.8739405652284622\n",
      "  batch 6000 loss: 0.8737449594140053\n",
      "  batch 7000 loss: 0.8747285939455033\n",
      "  batch 8000 loss: 0.8728559539914131\n",
      "  batch 9000 loss: 0.8734861931204796\n",
      "  batch 10000 loss: 0.8753460713028908\n",
      "  batch 11000 loss: 0.8681159091591835\n",
      "  batch 12000 loss: 0.863471664249897\n",
      "  batch 13000 loss: 0.8780975767970085\n",
      "  batch 14000 loss: 0.8697119861841202\n",
      "  batch 15000 loss: 0.862930976986885\n",
      "  batch 16000 loss: 0.8692204593420029\n",
      "  batch 17000 loss: 0.8676553362309932\n",
      "  batch 18000 loss: 0.8723405104875565\n",
      "  batch 19000 loss: 0.8710980111956597\n",
      "  batch 20000 loss: 0.8666689695119858\n",
      "  batch 21000 loss: 0.8679892126321792\n",
      "  batch 22000 loss: 0.8705052568316459\n",
      "  batch 23000 loss: 0.869565761744976\n",
      "  batch 24000 loss: 0.8643063570857048\n",
      "  batch 25000 loss: 0.8725719808340072\n",
      "  batch 26000 loss: 0.8665090207159519\n",
      "  batch 27000 loss: 0.8636379524469375\n",
      "  batch 28000 loss: 0.8682666031122208\n",
      "  batch 29000 loss: 0.8643168395161629\n",
      "LOSS train 0.8643168395161629 valid 0.22425857186317444\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.8596591091156006\n",
      "  batch 2000 loss: 0.8648254418373108\n",
      "  batch 3000 loss: 0.8650288642048836\n",
      "  batch 4000 loss: 0.867048397988081\n",
      "  batch 5000 loss: 0.8754968083202839\n",
      "  batch 6000 loss: 0.8742253012061119\n",
      "  batch 7000 loss: 0.8735615927577018\n",
      "  batch 8000 loss: 0.8779486417174339\n",
      "  batch 9000 loss: 0.8672060880064965\n",
      "  batch 10000 loss: 0.8739877514243126\n",
      "  batch 11000 loss: 0.8639101229310036\n",
      "  batch 12000 loss: 0.8695074373483658\n",
      "  batch 13000 loss: 0.8807717753052712\n",
      "  batch 14000 loss: 0.8728541321754456\n",
      "  batch 15000 loss: 0.8651575967669487\n",
      "  batch 16000 loss: 0.8711121463775635\n",
      "  batch 17000 loss: 0.8726404167711734\n",
      "  batch 18000 loss: 0.8734903106987476\n",
      "  batch 19000 loss: 0.8739900856912136\n",
      "  batch 20000 loss: 0.8705922267436981\n",
      "  batch 21000 loss: 0.8659852528572083\n",
      "  batch 22000 loss: 0.869666014790535\n",
      "  batch 23000 loss: 0.8710348592996597\n",
      "  batch 24000 loss: 0.8659206288456917\n",
      "  batch 25000 loss: 0.8726363333463669\n",
      "  batch 26000 loss: 0.8694901722073555\n",
      "  batch 27000 loss: 0.860861878991127\n",
      "  batch 28000 loss: 0.8763967135548592\n",
      "  batch 29000 loss: 0.8651472115516663\n",
      "LOSS train 0.8651472115516663 valid 0.1855902373790741\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model to reproducing melting temperatures of sequences with length between 6 and 9, the L1 loss gets to `0.197`. Not bad, given the wide range of melting temperatures observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot encoded base-pair steps fed to a fully connected network\n",
    "The columns of the `df` DataFrame contain the bases in each position of a sequence. Here, we need a DataFrame with columns containing the base-pair steps of each sequence. A base-pair step is defined by a list of contiguous bases along the sequence. For example, the sequence ATGC has base-pair steps AT, TG, and GC. Encoding the sequences like this might help, because in the SantaLucia model the melting temperature is computed from experimental coefficients associated to the base-pair step, rather than to the single bases.\n",
    "\n",
    "Let's generate a new dataframe, copied from `df` but with added columns for the base-pair steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0</th>\n",
       "      <th>base_1</th>\n",
       "      <th>base_2</th>\n",
       "      <th>base_3</th>\n",
       "      <th>base_4</th>\n",
       "      <th>base_5</th>\n",
       "      <th>...</th>\n",
       "      <th>base_7</th>\n",
       "      <th>base_8</th>\n",
       "      <th>base_01</th>\n",
       "      <th>base_12</th>\n",
       "      <th>base_23</th>\n",
       "      <th>base_34</th>\n",
       "      <th>base_45</th>\n",
       "      <th>base_56</th>\n",
       "      <th>base_67</th>\n",
       "      <th>base_78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               seq  s_mol  c_umol        T_m base_0 base_1 base_2 base_3  \\\n",
       "0           AAAAAA    0.1       1 -20.495855      A      A      A      A   \n",
       "1           AAAAAA    0.2       1 -17.536560      A      A      A      A   \n",
       "2           AAAAAA    0.5       1 -13.516521      A      A      A      A   \n",
       "3           AAAAAA    0.1       2 -17.837395      A      A      A      A   \n",
       "4           AAAAAA    0.2       2 -14.815124      A      A      A      A   \n",
       "...            ...    ...     ...        ...    ...    ...    ...    ...   \n",
       "2088955  GGGGGGGGG    0.2       1  47.545073      G      G      G      G   \n",
       "2088956  GGGGGGGGG    0.5       1  52.512866      G      G      G      G   \n",
       "2088957  GGGGGGGGG    0.1       2  46.070145      G      G      G      G   \n",
       "2088958  GGGGGGGGG    0.2       2  49.779454      G      G      G      G   \n",
       "2088959  GGGGGGGGG    0.5       2  54.817256      G      G      G      G   \n",
       "\n",
       "        base_4 base_5  ... base_7 base_8 base_01 base_12 base_23 base_34  \\\n",
       "0            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "1            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "2            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "3            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "4            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "...        ...    ...  ...    ...    ...     ...     ...     ...     ...   \n",
       "2088955      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088956      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088957      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088958      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088959      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "\n",
       "        base_45 base_56 base_67 base_78  \n",
       "0            AA      A0      00      00  \n",
       "1            AA      A0      00      00  \n",
       "2            AA      A0      00      00  \n",
       "3            AA      A0      00      00  \n",
       "4            AA      A0      00      00  \n",
       "...         ...     ...     ...     ...  \n",
       "2088955      GG      GG      GG      GG  \n",
       "2088956      GG      GG      GG      GG  \n",
       "2088957      GG      GG      GG      GG  \n",
       "2088958      GG      GG      GG      GG  \n",
       "2088959      GG      GG      GG      GG  \n",
       "\n",
       "[2088960 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bps = df.copy()\n",
    "i = 0\n",
    "while True:\n",
    "    base = 'base_'\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df[base + str(i + 1)]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # concatenate two columns to make base-pair steps. The 'astype(str)' are necessary to deal with the number-like 0 values in df_bps.\n",
    "    df_bps[base + str(i) + str(i + 1)] = df[base + str(i)].astype(str) + df[base + str(i+1)].astype(str)\n",
    "    i += 1    \n",
    "df_bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dataframe with one-hot encoded columns, similarly to how we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_01_AA</th>\n",
       "      <th>base_01_AC</th>\n",
       "      <th>base_01_AG</th>\n",
       "      <th>base_01_AT</th>\n",
       "      <th>base_01_CA</th>\n",
       "      <th>base_01_CC</th>\n",
       "      <th>base_01_CG</th>\n",
       "      <th>...</th>\n",
       "      <th>base_78_G0</th>\n",
       "      <th>base_78_GA</th>\n",
       "      <th>base_78_GC</th>\n",
       "      <th>base_78_GG</th>\n",
       "      <th>base_78_GT</th>\n",
       "      <th>base_78_T0</th>\n",
       "      <th>base_78_TA</th>\n",
       "      <th>base_78_TC</th>\n",
       "      <th>base_78_TG</th>\n",
       "      <th>base_78_TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         s_mol  c_umol        T_m  base_01_AA  base_01_AC  base_01_AG  \\\n",
       "0          0.1       1 -20.495855           1           0           0   \n",
       "1          0.2       1 -17.536560           1           0           0   \n",
       "2          0.5       1 -13.516521           1           0           0   \n",
       "3          0.1       2 -17.837395           1           0           0   \n",
       "4          0.2       2 -14.815124           1           0           0   \n",
       "...        ...     ...        ...         ...         ...         ...   \n",
       "2088955    0.2       1  47.545073           0           0           0   \n",
       "2088956    0.5       1  52.512866           0           0           0   \n",
       "2088957    0.1       2  46.070145           0           0           0   \n",
       "2088958    0.2       2  49.779454           0           0           0   \n",
       "2088959    0.5       2  54.817256           0           0           0   \n",
       "\n",
       "         base_01_AT  base_01_CA  base_01_CC  base_01_CG  ...  base_78_G0  \\\n",
       "0                 0           0           0           0  ...           0   \n",
       "1                 0           0           0           0  ...           0   \n",
       "2                 0           0           0           0  ...           0   \n",
       "3                 0           0           0           0  ...           0   \n",
       "4                 0           0           0           0  ...           0   \n",
       "...             ...         ...         ...         ...  ...         ...   \n",
       "2088955           0           0           0           0  ...           0   \n",
       "2088956           0           0           0           0  ...           0   \n",
       "2088957           0           0           0           0  ...           0   \n",
       "2088958           0           0           0           0  ...           0   \n",
       "2088959           0           0           0           0  ...           0   \n",
       "\n",
       "         base_78_GA  base_78_GC  base_78_GG  base_78_GT  base_78_T0  \\\n",
       "0                 0           0           0           0           0   \n",
       "1                 0           0           0           0           0   \n",
       "2                 0           0           0           0           0   \n",
       "3                 0           0           0           0           0   \n",
       "4                 0           0           0           0           0   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "2088955           0           0           1           0           0   \n",
       "2088956           0           0           1           0           0   \n",
       "2088957           0           0           1           0           0   \n",
       "2088958           0           0           1           0           0   \n",
       "2088959           0           0           1           0           0   \n",
       "\n",
       "         base_78_TA  base_78_TC  base_78_TG  base_78_TT  \n",
       "0                 0           0           0           0  \n",
       "1                 0           0           0           0  \n",
       "2                 0           0           0           0  \n",
       "3                 0           0           0           0  \n",
       "4                 0           0           0           0  \n",
       "...             ...         ...         ...         ...  \n",
       "2088955           0           0           0           0  \n",
       "2088956           0           0           0           0  \n",
       "2088957           0           0           0           0  \n",
       "2088958           0           0           0           0  \n",
       "2088959           0           0           0           0  \n",
       "\n",
       "[2088960 rows x 145 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bps_oh = df_bps.copy()\n",
    "i = 0\n",
    "while True:\n",
    "    base = 'base_'\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df_bps[base + str(i + 1)]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # get new one-hot encoded columns\n",
    "    target_col = base + str(i) + str(i + 1)\n",
    "    new_cols = pd.get_dummies(df_bps[target_col], prefix = target_col)\n",
    "    # concatenate them to the dataframe and drop the original columns\n",
    "    df_bps_oh = pd.concat([df_bps_oh, new_cols], axis=1)\n",
    "    df_bps_oh.drop([target_col, base + str(i)], axis=1, inplace=True)\n",
    "    i += 1\n",
    "df_bps_oh.drop(['seq', base + str(i)], axis=1, inplace=True)\n",
    "df_bps_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the variables are encoded, we can load the data into dataloader objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1880064 instances\n",
      "Validation set has 208896 instances\n"
     ]
    }
   ],
   "source": [
    "# Cast the data into a dataset\n",
    "y_bps = torch.Tensor(df_bps.T_m)\n",
    "x_bps = torch.Tensor(df_bps_oh.drop('T_m', axis=1).values)\n",
    "\n",
    "dataset_bps = TensorDataset(x_bps, y_bps)\n",
    "\n",
    "train_length_bps = int(len(dataset_bps) * 0.9)\n",
    "valid_length_bps = len(dataset_bps) - train_length_bps\n",
    "train_set_bps, valid_set_bps = torch.utils.data.random_split(dataset_bps, [train_length_bps, valid_length_bps])\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_set_bps)} instances')\n",
    "print(f'Validation set has {len(valid_set_bps)} instances')\n",
    "\n",
    "# Create the loaders\n",
    "batch_size_bps = 64\n",
    "training_loader_bps = DataLoader(train_set_bps, batch_size=batch_size_bps, shuffle=False, num_workers=2)\n",
    "validation_loader_bps = DataLoader(valid_set_bps, batch_size=batch_size_bps, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model structure, loss and optimizer are the same as in the previous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bps = FC(x_bps.shape[1], 1, x_bps.shape[1] * 2)\n",
    "#loss_fn_bps = nn.MSELoss()\n",
    "loss_fn_bps = nn.L1Loss()\n",
    "optimizer_bps = torch.optim.Adam(model_bps.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite a training function, `train_one_epoch_bps` just renaming some of the variable it uses from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_bps(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader_bps):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_bps.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_bps(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "        loss = loss_fn_bps(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_bps.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader_bps) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.6668476646840572\n",
      "  batch 2000 loss: 0.6710153155326843\n",
      "  batch 3000 loss: 0.6643780255913735\n",
      "  batch 4000 loss: 0.6670621097683906\n",
      "  batch 5000 loss: 0.6721961503028869\n",
      "  batch 6000 loss: 0.6631925288736821\n",
      "  batch 7000 loss: 0.6651138181984425\n",
      "  batch 8000 loss: 0.6696283655166626\n",
      "  batch 9000 loss: 0.6626340038180352\n",
      "  batch 10000 loss: 0.6651555118262767\n",
      "  batch 11000 loss: 0.6601477321386338\n",
      "  batch 12000 loss: 0.6683665881156922\n",
      "  batch 13000 loss: 0.6631973322629928\n",
      "  batch 14000 loss: 0.6603917090892791\n",
      "  batch 15000 loss: 0.6674748746752739\n",
      "  batch 16000 loss: 0.6658297376930714\n",
      "  batch 17000 loss: 0.6695267750322819\n",
      "  batch 18000 loss: 0.6668776022791862\n",
      "  batch 19000 loss: 0.6682028940021991\n",
      "  batch 20000 loss: 0.6686720502078534\n",
      "  batch 21000 loss: 0.6551470183730126\n",
      "  batch 22000 loss: 0.6630631284713745\n",
      "  batch 23000 loss: 0.6692916398048401\n",
      "  batch 24000 loss: 0.6677564569413662\n",
      "  batch 25000 loss: 0.6666619502604008\n",
      "  batch 26000 loss: 0.6645305279195308\n",
      "  batch 27000 loss: 0.6667477128207684\n",
      "  batch 28000 loss: 0.662334054261446\n",
      "  batch 29000 loss: 0.6654305509328842\n",
      "LOSS train 0.6654305509328842 valid 0.16445083916187286\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.6654229079782963\n",
      "  batch 2000 loss: 0.6669612393081188\n",
      "  batch 3000 loss: 0.6693050451874732\n",
      "  batch 4000 loss: 0.6636647310256958\n",
      "  batch 5000 loss: 0.6666027408540249\n",
      "  batch 6000 loss: 0.6581841943860054\n",
      "  batch 7000 loss: 0.6612280113399028\n",
      "  batch 8000 loss: 0.6666564337611198\n",
      "  batch 9000 loss: 0.661250153630972\n",
      "  batch 10000 loss: 0.6618150877952576\n",
      "  batch 11000 loss: 0.6623718938529491\n",
      "  batch 12000 loss: 0.6586522225737572\n",
      "  batch 13000 loss: 0.6557437892258168\n",
      "  batch 14000 loss: 0.6607693812549115\n",
      "  batch 15000 loss: 0.6645264014303685\n",
      "  batch 16000 loss: 0.6668860068619251\n",
      "  batch 17000 loss: 0.6612784525454044\n",
      "  batch 18000 loss: 0.6637227689623832\n",
      "  batch 19000 loss: 0.6672006106078625\n",
      "  batch 20000 loss: 0.6588950991332531\n",
      "  batch 21000 loss: 0.6605498000085354\n",
      "  batch 22000 loss: 0.6664812179505825\n",
      "  batch 23000 loss: 0.6656847477555276\n",
      "  batch 24000 loss: 0.6649298889935017\n",
      "  batch 25000 loss: 0.6599014852941036\n",
      "  batch 26000 loss: 0.6602688911855221\n",
      "  batch 27000 loss: 0.6611641104221344\n",
      "  batch 28000 loss: 0.6614069855213165\n",
      "  batch 29000 loss: 0.6604600958824157\n",
      "LOSS train 0.6604600958824157 valid 0.22841621935367584\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.6602880024313926\n",
      "  batch 2000 loss: 0.6668275547623634\n",
      "  batch 3000 loss: 0.6625435158014298\n",
      "  batch 4000 loss: 0.6645554772317409\n",
      "  batch 5000 loss: 0.6623041985630989\n",
      "  batch 6000 loss: 0.659381699860096\n",
      "  batch 7000 loss: 0.6643270887732505\n",
      "  batch 8000 loss: 0.6635086804926396\n",
      "  batch 9000 loss: 0.6638423131704331\n",
      "  batch 10000 loss: 0.6578381116092205\n",
      "  batch 11000 loss: 0.6597448832690715\n",
      "  batch 12000 loss: 0.6616827535629273\n",
      "  batch 13000 loss: 0.6559041710495949\n",
      "  batch 14000 loss: 0.6574110115766525\n",
      "  batch 15000 loss: 0.6660654518306255\n",
      "  batch 16000 loss: 0.6595614856779576\n",
      "  batch 17000 loss: 0.6616975508332252\n",
      "  batch 18000 loss: 0.6540492628216743\n",
      "  batch 19000 loss: 0.660412660241127\n",
      "  batch 20000 loss: 0.6622159686684609\n",
      "  batch 21000 loss: 0.6581995126008987\n",
      "  batch 22000 loss: 0.6618592222034931\n",
      "  batch 23000 loss: 0.6626405720114708\n",
      "  batch 24000 loss: 0.6592616689801216\n",
      "  batch 25000 loss: 0.6613767091929913\n",
      "  batch 26000 loss: 0.6577510079443455\n",
      "  batch 27000 loss: 0.6593276326954365\n",
      "  batch 28000 loss: 0.6605435326993465\n",
      "  batch 29000 loss: 0.660018334299326\n",
      "LOSS train 0.660018334299326 valid 0.189432755112648\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.6548434300124645\n",
      "  batch 2000 loss: 0.6585173230171204\n",
      "  batch 3000 loss: 0.6601128925085068\n",
      "  batch 4000 loss: 0.657492768585682\n",
      "  batch 5000 loss: 0.6560164052546025\n",
      "  batch 6000 loss: 0.6566075146198272\n",
      "  batch 7000 loss: 0.6584895112216472\n",
      "  batch 8000 loss: 0.6543658170998097\n",
      "  batch 9000 loss: 0.6557635025978088\n",
      "  batch 10000 loss: 0.65771635851264\n",
      "  batch 11000 loss: 0.6544254512190819\n",
      "  batch 12000 loss: 0.6575043424665928\n",
      "  batch 13000 loss: 0.6557385226488114\n",
      "  batch 14000 loss: 0.6571964591443539\n",
      "  batch 15000 loss: 0.6641605105698108\n",
      "  batch 16000 loss: 0.6581628378629685\n",
      "  batch 17000 loss: 0.6557188465595245\n",
      "  batch 18000 loss: 0.6612575619220734\n",
      "  batch 19000 loss: 0.6618950131833553\n",
      "  batch 20000 loss: 0.6559078300595284\n",
      "  batch 21000 loss: 0.6566490587890148\n",
      "  batch 22000 loss: 0.6581086274683475\n",
      "  batch 23000 loss: 0.6587540351450444\n",
      "  batch 24000 loss: 0.6593456777632236\n",
      "  batch 25000 loss: 0.6567605301737786\n",
      "  batch 26000 loss: 0.6549073109924793\n",
      "  batch 27000 loss: 0.659572276264429\n",
      "  batch 28000 loss: 0.6599891946911812\n",
      "  batch 29000 loss: 0.6588781196177006\n",
      "LOSS train 0.6588781196177006 valid 0.18099071085453033\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.6511115336716176\n",
      "  batch 2000 loss: 0.6569203209280968\n",
      "  batch 3000 loss: 0.6594657965898514\n",
      "  batch 4000 loss: 0.661553583920002\n",
      "  batch 5000 loss: 0.6562833194136619\n",
      "  batch 6000 loss: 0.6524740453064442\n",
      "  batch 7000 loss: 0.6578824400305748\n",
      "  batch 8000 loss: 0.6572208218872547\n",
      "  batch 9000 loss: 0.6583391485810279\n",
      "  batch 10000 loss: 0.6576585295796394\n",
      "  batch 11000 loss: 0.6553575252592564\n",
      "  batch 12000 loss: 0.6566097317039966\n",
      "  batch 13000 loss: 0.653102182328701\n",
      "  batch 14000 loss: 0.6541490536928177\n",
      "  batch 15000 loss: 0.6569586248099804\n",
      "  batch 16000 loss: 0.6519471380412578\n",
      "  batch 17000 loss: 0.6550043882429599\n",
      "  batch 18000 loss: 0.6577203842103482\n",
      "  batch 19000 loss: 0.6590088073015213\n",
      "  batch 20000 loss: 0.6576881827712059\n",
      "  batch 21000 loss: 0.6507599720358849\n",
      "  batch 22000 loss: 0.6610298046469688\n",
      "  batch 23000 loss: 0.6554301565885544\n",
      "  batch 24000 loss: 0.6606515457332134\n",
      "  batch 25000 loss: 0.6533216139674187\n",
      "  batch 26000 loss: 0.6589956867098808\n",
      "  batch 27000 loss: 0.6558524181246758\n",
      "  batch 28000 loss: 0.6549391009509563\n",
      "  batch 29000 loss: 0.6529877307415008\n",
      "LOSS train 0.6529877307415008 valid 0.2521541714668274\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_bps.train(True)\n",
    "    avg_loss = train_one_epoch_bps(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_bps.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader_bps):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_bps(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_bps(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_bps = 'model_bps_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_bps.state_dict(), model_path_bps)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the L1 loss decreases slightly to `0.164`. Let's see if we can get even lower than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoded bases fed to a recurring neural network and fully connected network\n",
    "The DNA sequence can be expressed as a string, so it feels natural to try and tackle this problem with a recurring neural network. Let's give it a shot!\n",
    "\n",
    "We can reuse the dataloaders from point 1 above, so there is no need to define new dataloaders here.\n",
    "\n",
    "On the other hand, we used a flattened one-hot encoding in both the models above, but in order to use the recurring neural network as encoded in the `nn.RNN` module we will need to store the one-hot encoded variables into a tensor with shape `(batch_size, seq_len, n_bases)`. The other two variables (duplex and salt concentrations) can be fed as they are to the fully-connected network. Let's write a function to convert the one-hot variables into a tensor, to be called from within the `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bases = 5 # 4 possible letters among A, T, C, G, and 0 for no base.\n",
    "def get_conc_features(x):\n",
    "    '''\n",
    "    Given a batch of data x, return:\n",
    "    1. a tensor of shape (batch_size, 2) containing the concentrations, and\n",
    "    2. a tensor of shape (batch_size, seq_len, n_bases) containing the one-hot encoded sequence features\n",
    "    '''\n",
    "    concs = x[:,:2]\n",
    "    oh_flat = x[:, 2:]\n",
    "    seq_len = df.shape[1] - 4# this uses the unprocessed dataframe read from file\n",
    "    \n",
    "    oh = torch.reshape(oh_flat, (x.shape[0],seq_len, n_bases))\n",
    "\n",
    "    return concs, oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model that we're going to use: a simple recurring neural network, whose output is fed to a fully connected network together with duplex and salt concentrations. We also define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, seq_len):\n",
    "        super(RNNFC, self).__init__()\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        n_conc = 2\n",
    "        fc_hidden_dim = hidden_dim * seq_len + n_conc\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        # using relu nonlinearity to prevent vanishing gradient problems\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, batch_first=True, nonlinearity='relu')   \n",
    "        # Fully connected layers, receiving the output of rnn and the two concentrations\n",
    "        self.fc1 = nn.Linear(fc_hidden_dim, fc_hidden_dim)\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Split the input tensor between concentrations and one-hot encoded variables\n",
    "        concs, features = get_conc_features(x)\n",
    "\n",
    "        # Passing features into the rnn\n",
    "        # no need to pass a hidden state as it will be initialized to zero by default\n",
    "        out, hidden = self.rnn(features)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(batch_size, -1)\n",
    "        \n",
    "        # concatenating the outputs to the concentrations and feeding them to the\n",
    "        # fully connected layers\n",
    "        out = torch.cat((concs, out), axis=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "seq_len = df.shape[1] - 4\n",
    "model_rnn = RNNFC(n_bases, 3, seq_len)\n",
    "#loss_fn_rnn = nn.MSELoss()\n",
    "loss_fn_rnn = nn.L1Loss()\n",
    "optimizer_rnn = torch.optim.Adam(model_rnn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also rewrite the training function, `train_one_epoch_rnn`, similarly to how we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_rnn(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_rnn.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_rnn(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        loss = loss_fn_rnn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.7829884619116784\n",
      "  batch 2000 loss: 0.7846234810352325\n",
      "  batch 3000 loss: 0.7895395053029061\n",
      "  batch 4000 loss: 0.7794713680744171\n",
      "  batch 5000 loss: 0.7899159904122353\n",
      "  batch 6000 loss: 0.7815228298306465\n",
      "  batch 7000 loss: 0.7823211649656295\n",
      "  batch 8000 loss: 0.7856781156659126\n",
      "  batch 9000 loss: 0.7814884135723114\n",
      "  batch 10000 loss: 0.7795448210239411\n",
      "  batch 11000 loss: 0.7837608658671379\n",
      "  batch 12000 loss: 0.7838066540360451\n",
      "  batch 13000 loss: 0.782602298617363\n",
      "  batch 14000 loss: 0.7831559042930603\n",
      "  batch 15000 loss: 0.7836108455657959\n",
      "  batch 16000 loss: 0.7795608876943588\n",
      "  batch 17000 loss: 0.7803429078459739\n",
      "  batch 18000 loss: 0.7845448657870293\n",
      "  batch 19000 loss: 0.7807118447422982\n",
      "  batch 20000 loss: 0.7791956310868263\n",
      "  batch 21000 loss: 0.7817626765966416\n",
      "  batch 22000 loss: 0.7796033869981766\n",
      "  batch 23000 loss: 0.7839045576453209\n",
      "  batch 24000 loss: 0.7811657485365867\n",
      "  batch 25000 loss: 0.7756195151209831\n",
      "  batch 26000 loss: 0.7777869108915328\n",
      "  batch 27000 loss: 0.7801786568164826\n",
      "  batch 28000 loss: 0.7776886144280434\n",
      "  batch 29000 loss: 0.7715965066552162\n",
      "LOSS train 0.7715965066552162 valid 0.7707129716873169\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.7755288049578667\n",
      "  batch 2000 loss: 0.7748215977549553\n",
      "  batch 3000 loss: 0.7818808539509773\n",
      "  batch 4000 loss: 0.769042965054512\n",
      "  batch 5000 loss: 0.7801137925982475\n",
      "  batch 6000 loss: 0.7724807050824165\n",
      "  batch 7000 loss: 0.7715864450335502\n",
      "  batch 8000 loss: 0.7755025057196617\n",
      "  batch 9000 loss: 0.7725539940595627\n",
      "  batch 10000 loss: 0.7673622522950172\n",
      "  batch 11000 loss: 0.7709417824745178\n",
      "  batch 12000 loss: 0.7727061477899552\n",
      "  batch 13000 loss: 0.7682150624990464\n",
      "  batch 14000 loss: 0.7684951485395431\n",
      "  batch 15000 loss: 0.7651546899080276\n",
      "  batch 16000 loss: 0.7610029374361038\n",
      "  batch 17000 loss: 0.7574913946092129\n",
      "  batch 18000 loss: 0.759063294172287\n",
      "  batch 19000 loss: 0.7546343050003052\n",
      "  batch 20000 loss: 0.7527924174666405\n",
      "  batch 21000 loss: 0.7538525058031083\n",
      "  batch 22000 loss: 0.7542131744027137\n",
      "  batch 23000 loss: 0.7552063018679619\n",
      "  batch 24000 loss: 0.7512828511893749\n",
      "  batch 25000 loss: 0.7495024519562721\n",
      "  batch 26000 loss: 0.7486163426041603\n",
      "  batch 27000 loss: 0.751710745871067\n",
      "  batch 28000 loss: 0.746758115708828\n",
      "  batch 29000 loss: 0.741688244342804\n",
      "LOSS train 0.741688244342804 valid 0.7417019009590149\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.7440191432237625\n",
      "  batch 2000 loss: 0.7432623429894447\n",
      "  batch 3000 loss: 0.7516436221599578\n",
      "  batch 4000 loss: 0.7387546489238739\n",
      "  batch 5000 loss: 0.7499041410088539\n",
      "  batch 6000 loss: 0.7419839476943016\n",
      "  batch 7000 loss: 0.7443452771902085\n",
      "  batch 8000 loss: 0.7453305723965168\n",
      "  batch 9000 loss: 0.7434178843796253\n",
      "  batch 10000 loss: 0.7396249026656151\n",
      "  batch 11000 loss: 0.742180091559887\n",
      "  batch 12000 loss: 0.743536091208458\n",
      "  batch 13000 loss: 0.7392664428949356\n",
      "  batch 14000 loss: 0.7429677028656005\n",
      "  batch 15000 loss: 0.7393778653144837\n",
      "  batch 16000 loss: 0.7403236372470856\n",
      "  batch 17000 loss: 0.7384156717956066\n",
      "  batch 18000 loss: 0.737883293747902\n",
      "  batch 19000 loss: 0.7346121546626091\n",
      "  batch 20000 loss: 0.733415659725666\n",
      "  batch 21000 loss: 0.7350374935865402\n",
      "  batch 22000 loss: 0.7339037841558457\n",
      "  batch 23000 loss: 0.7354988193213939\n",
      "  batch 24000 loss: 0.7325202176868916\n",
      "  batch 25000 loss: 0.7302230040133\n",
      "  batch 26000 loss: 0.7312106450796128\n",
      "  batch 27000 loss: 0.7362815083265305\n",
      "  batch 28000 loss: 0.7300597787499428\n",
      "  batch 29000 loss: 0.7247010608315467\n",
      "LOSS train 0.7247010608315467 valid 0.725688636302948\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.7276481559276581\n",
      "  batch 2000 loss: 0.7270361227989197\n",
      "  batch 3000 loss: 0.7343938806354999\n",
      "  batch 4000 loss: 0.7256986023783684\n",
      "  batch 5000 loss: 0.7340835427641869\n",
      "  batch 6000 loss: 0.7270563658773899\n",
      "  batch 7000 loss: 0.729593167245388\n",
      "  batch 8000 loss: 0.7315208646059036\n",
      "  batch 9000 loss: 0.7290538526177406\n",
      "  batch 10000 loss: 0.7256326176524163\n",
      "  batch 11000 loss: 0.7281155574917794\n",
      "  batch 12000 loss: 0.7300067463517189\n",
      "  batch 13000 loss: 0.7266907150745392\n",
      "  batch 14000 loss: 0.7290710589587689\n",
      "  batch 15000 loss: 0.7256338177919388\n",
      "  batch 16000 loss: 0.7297405967712403\n",
      "  batch 17000 loss: 0.7292142704427242\n",
      "  batch 18000 loss: 0.729922962397337\n",
      "  batch 19000 loss: 0.7297754485011101\n",
      "  batch 20000 loss: 0.7259672876298428\n",
      "  batch 21000 loss: 0.7293893323242664\n",
      "  batch 22000 loss: 0.7274030054807663\n",
      "  batch 23000 loss: 0.7304583490490913\n",
      "  batch 24000 loss: 0.7274899227321148\n",
      "  batch 25000 loss: 0.7268897035121917\n",
      "  batch 26000 loss: 0.7250070036351681\n",
      "  batch 27000 loss: 0.7293320743441581\n",
      "  batch 28000 loss: 0.7255492312908173\n",
      "  batch 29000 loss: 0.7211983671784401\n",
      "LOSS train 0.7211983671784401 valid 0.7196628451347351\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.723485801011324\n",
      "  batch 2000 loss: 0.7219099678993225\n",
      "  batch 3000 loss: 0.7307706111669541\n",
      "  batch 4000 loss: 0.7212049661874771\n",
      "  batch 5000 loss: 0.7310410912632942\n",
      "  batch 6000 loss: 0.7229663551747799\n",
      "  batch 7000 loss: 0.7252212129533291\n",
      "  batch 8000 loss: 0.7283950829207897\n",
      "  batch 9000 loss: 0.7254623987674713\n",
      "  batch 10000 loss: 0.7219708126187324\n",
      "  batch 11000 loss: 0.7252026990354061\n",
      "  batch 12000 loss: 0.7257379401922226\n",
      "  batch 13000 loss: 0.7240605007410049\n",
      "  batch 14000 loss: 0.7242878570854664\n",
      "  batch 15000 loss: 0.7208928588032723\n",
      "  batch 16000 loss: 0.726593022108078\n",
      "  batch 17000 loss: 0.7275166491270065\n",
      "  batch 18000 loss: 0.7263499247133732\n",
      "  batch 19000 loss: 0.7266556579470634\n",
      "  batch 20000 loss: 0.7241930320262909\n",
      "  batch 21000 loss: 0.7251072623729706\n",
      "  batch 22000 loss: 0.7242198837995529\n",
      "  batch 23000 loss: 0.7267050431966782\n",
      "  batch 24000 loss: 0.724379900842905\n",
      "  batch 25000 loss: 0.7222031456232071\n",
      "  batch 26000 loss: 0.7224072031378747\n",
      "  batch 27000 loss: 0.7268059344291687\n",
      "  batch 28000 loss: 0.7225793153643608\n",
      "  batch 29000 loss: 0.718788145005703\n",
      "LOSS train 0.718788145005703 valid 0.7190206050872803\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_rnn.train(True)\n",
    "    avg_loss = train_one_epoch_rnn(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_rnn.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_rnn(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_rnn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_rnn = 'model_rnn_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_rnn.state_dict(), model_path_rnn)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us to `0.719`. Training time on this model is much longer than on the previous two models, therefore we have not seen the loss getting to a minimum yet. It is therefore possible that training for even longer could give us a lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoded bases fed to a recurring neural network and fully connected network\n",
    "\n",
    "Finally, we try replacing the RNN layer with a LSTM layer, a more elaborate kind of recurring neural network.\n",
    "\n",
    "To do so, we can keep the same loss and optimizer, and even reuse the `Model` class we used for the previous model: we just have to redefine its `rnn` layer to use a `LTSM` module rather than an `RNN` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = RNNFC(n_bases, 3, seq_len)\n",
    "model_lstm.rnn = nn.LSTM(n_bases, 3, batch_first=True)\n",
    "#loss_fn_lstm = nn.MSELoss()\n",
    "loss_fn_lstm = nn.L1Loss()\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_lstm(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_lstm.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_lstm(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        loss = loss_fn_lstm(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.13859391354769468\n",
      "  batch 2000 loss: 0.13823284746706485\n",
      "  batch 3000 loss: 0.1378572311028838\n",
      "  batch 4000 loss: 0.13808697562664748\n",
      "  batch 5000 loss: 0.1382684726268053\n",
      "  batch 6000 loss: 0.1391499951854348\n",
      "  batch 7000 loss: 0.13625500805675983\n",
      "  batch 8000 loss: 0.13720592268556356\n",
      "  batch 9000 loss: 0.13538634381443262\n",
      "  batch 10000 loss: 0.1382701846882701\n",
      "  batch 11000 loss: 0.13892840971052647\n",
      "  batch 12000 loss: 0.1360404548868537\n",
      "  batch 13000 loss: 0.1376344921886921\n",
      "  batch 14000 loss: 0.136000742174685\n",
      "  batch 15000 loss: 0.13663926880806684\n",
      "  batch 16000 loss: 0.13557884684205054\n",
      "  batch 17000 loss: 0.13646392888575792\n",
      "  batch 18000 loss: 0.13647949022054673\n",
      "  batch 19000 loss: 0.13516800666600465\n",
      "  batch 20000 loss: 0.1357798506990075\n",
      "  batch 21000 loss: 0.13491579384356736\n",
      "  batch 22000 loss: 0.13463738422095775\n",
      "  batch 23000 loss: 0.1358866397663951\n",
      "  batch 24000 loss: 0.1338921784609556\n",
      "  batch 25000 loss: 0.1350943088606\n",
      "  batch 26000 loss: 0.13487543312460185\n",
      "  batch 27000 loss: 0.13457761757075787\n",
      "  batch 28000 loss: 0.13341637061536313\n",
      "  batch 29000 loss: 0.13504918114840983\n",
      "LOSS train 0.13504918114840983 valid 0.1308377981185913\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.13557938773930073\n",
      "  batch 2000 loss: 0.13510824239999056\n",
      "  batch 3000 loss: 0.1345141823589802\n",
      "  batch 4000 loss: 0.13512882856279612\n",
      "  batch 5000 loss: 0.13532613077014685\n",
      "  batch 6000 loss: 0.13530407281219958\n",
      "  batch 7000 loss: 0.1331502329930663\n",
      "  batch 8000 loss: 0.13510735292732715\n",
      "  batch 9000 loss: 0.13305003222078085\n",
      "  batch 10000 loss: 0.1358957938924432\n",
      "  batch 11000 loss: 0.13680403791368007\n",
      "  batch 12000 loss: 0.13309576059132813\n",
      "  batch 13000 loss: 0.13539801333844662\n",
      "  batch 14000 loss: 0.13289043253660202\n",
      "  batch 15000 loss: 0.13512654764205217\n",
      "  batch 16000 loss: 0.13335240440815688\n",
      "  batch 17000 loss: 0.1340699201747775\n",
      "  batch 18000 loss: 0.13415065948665142\n",
      "  batch 19000 loss: 0.13346231465786695\n",
      "  batch 20000 loss: 0.13395528731495143\n",
      "  batch 21000 loss: 0.13364647784084083\n",
      "  batch 22000 loss: 0.1325427568256855\n",
      "  batch 23000 loss: 0.13362871599197387\n",
      "  batch 24000 loss: 0.1321266945898533\n",
      "  batch 25000 loss: 0.1346947675049305\n",
      "  batch 26000 loss: 0.1326202722042799\n",
      "  batch 27000 loss: 0.1324703469723463\n",
      "  batch 28000 loss: 0.13189660976082088\n",
      "  batch 29000 loss: 0.13351632287353277\n",
      "LOSS train 0.13351632287353277 valid 0.13203567266464233\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.13387423427402972\n",
      "  batch 2000 loss: 0.13242993907630443\n",
      "  batch 3000 loss: 0.13257210029661656\n",
      "  batch 4000 loss: 0.1342402954697609\n",
      "  batch 5000 loss: 0.133099211551249\n",
      "  batch 6000 loss: 0.13338404723256828\n",
      "  batch 7000 loss: 0.1329848431572318\n",
      "  batch 8000 loss: 0.13270854791253806\n",
      "  batch 9000 loss: 0.1312742192968726\n",
      "  batch 10000 loss: 0.13393738452345133\n",
      "  batch 11000 loss: 0.13465989468991757\n",
      "  batch 12000 loss: 0.13216954358667135\n",
      "  batch 13000 loss: 0.13322887068241834\n",
      "  batch 14000 loss: 0.1315298547372222\n",
      "  batch 15000 loss: 0.13299568437039852\n",
      "  batch 16000 loss: 0.1316735629066825\n",
      "  batch 17000 loss: 0.13334905605018138\n",
      "  batch 18000 loss: 0.1329681381508708\n",
      "  batch 19000 loss: 0.131488905377686\n",
      "  batch 20000 loss: 0.13369163689017297\n",
      "  batch 21000 loss: 0.13129645960777997\n",
      "  batch 22000 loss: 0.13143607757240533\n",
      "  batch 23000 loss: 0.13295762841403486\n",
      "  batch 24000 loss: 0.132558121830225\n",
      "  batch 25000 loss: 0.13283175338059663\n",
      "  batch 26000 loss: 0.131727278791368\n",
      "  batch 27000 loss: 0.131619384765625\n",
      "  batch 28000 loss: 0.1318642908707261\n",
      "  batch 29000 loss: 0.13220524825155736\n",
      "LOSS train 0.13220524825155736 valid 0.13892333209514618\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.13165347554534673\n",
      "  batch 2000 loss: 0.1314876707792282\n",
      "  batch 3000 loss: 0.1313883213698864\n",
      "  batch 4000 loss: 0.1326980535313487\n",
      "  batch 5000 loss: 0.13195453716814518\n",
      "  batch 6000 loss: 0.133762408092618\n",
      "  batch 7000 loss: 0.1323994558751583\n",
      "  batch 8000 loss: 0.13099783419817687\n",
      "  batch 9000 loss: 0.1298638225570321\n",
      "  batch 10000 loss: 0.13370647621899842\n",
      "  batch 11000 loss: 0.13258636416494846\n",
      "  batch 12000 loss: 0.13061894706636668\n",
      "  batch 13000 loss: 0.13189415060728787\n",
      "  batch 14000 loss: 0.13072504260390996\n",
      "  batch 15000 loss: 0.1317580412477255\n",
      "  batch 16000 loss: 0.1306923873871565\n",
      "  batch 17000 loss: 0.13277419778704644\n",
      "  batch 18000 loss: 0.13203698530048133\n",
      "  batch 19000 loss: 0.13059276869148015\n",
      "  batch 20000 loss: 0.13274712528288365\n",
      "  batch 21000 loss: 0.13149642269313336\n",
      "  batch 22000 loss: 0.1296515022367239\n",
      "  batch 23000 loss: 0.13161327665299177\n",
      "  batch 24000 loss: 0.12936884820461272\n",
      "  batch 25000 loss: 0.13152576190978288\n",
      "  batch 26000 loss: 0.13062952361255883\n",
      "  batch 27000 loss: 0.12995775909721852\n",
      "  batch 28000 loss: 0.12976520949602127\n",
      "  batch 29000 loss: 0.13056708881258963\n",
      "LOSS train 0.13056708881258963 valid 0.1308472901582718\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.13122248687595128\n",
      "  batch 2000 loss: 0.1304002650603652\n",
      "  batch 3000 loss: 0.13028747325390577\n",
      "  batch 4000 loss: 0.1313478938192129\n",
      "  batch 5000 loss: 0.13127693405747415\n",
      "  batch 6000 loss: 0.131856652982533\n",
      "  batch 7000 loss: 0.12939225322008133\n",
      "  batch 8000 loss: 0.13072402130067348\n",
      "  batch 9000 loss: 0.12883121877163647\n",
      "  batch 10000 loss: 0.1312639673948288\n",
      "  batch 11000 loss: 0.13125068483501673\n",
      "  batch 12000 loss: 0.13013183425366878\n",
      "  batch 13000 loss: 0.13071363515406847\n",
      "  batch 14000 loss: 0.12964013344049455\n",
      "  batch 15000 loss: 0.13120075722783803\n",
      "  batch 16000 loss: 0.12932532549649478\n",
      "  batch 17000 loss: 0.13101116482913494\n",
      "  batch 18000 loss: 0.12981872189044952\n",
      "  batch 19000 loss: 0.12962826739251615\n",
      "  batch 20000 loss: 0.13153434377908707\n",
      "  batch 21000 loss: 0.1300635924860835\n",
      "  batch 22000 loss: 0.12977630291134118\n",
      "  batch 23000 loss: 0.13066355963051318\n",
      "  batch 24000 loss: 0.12909528862684966\n",
      "  batch 25000 loss: 0.1301943605914712\n",
      "  batch 26000 loss: 0.12967514704167843\n",
      "  batch 27000 loss: 0.1298334058895707\n",
      "  batch 28000 loss: 0.12845997958630323\n",
      "  batch 29000 loss: 0.1290739078745246\n",
      "LOSS train 0.1290739078745246 valid 0.128224715590477\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_lstm.train(True)\n",
    "    avg_loss = train_one_epoch_lstm(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_lstm.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_lstm(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_lstm(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_lstm = 'model_lstm_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_lstm.state_dict(), model_path_lstm)\n",
    "\n",
    "    epoch_number += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets the lowest loss of all, to `0.128`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "All the models perform fairly well, but the best one is the one is model #4, which uses an LSTM layer. As long as the ML model is trained on the output of the SantaLucia model, this work is rather pointless, but it could be valuable if we were to train it on a database of melting temperatures obtained in experiments: the SantaLucia model has been shown to differ from experimental data by as much as 2C, while more precise DNA melting temperature predictions would help to parametrise models for DNA nanotechnology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11a39a0d09cc5009ec4bb720bab51ac91bcdef19676420c2ad1822c8de114940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
