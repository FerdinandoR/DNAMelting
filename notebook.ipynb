{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA Melting\n",
    "A suite of several machine learning models to predict DNA melting temperature. For all of intents and purposes of this notebook, we only need to know that this is a number that depends on two other numbers (the duplex and salt concentrations) and on a string. Each string represents a DNA sequence, hence it is composed of A, C, G, T. Similar strings tend to have a similar melting temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import a few packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "import SantaLucia as sl\n",
    "import make_melting_database\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "bases = ['A', 'C', 'G', 'T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only for the first time that the notebook is run, or when we want to add/change the data stored in the melting temperature database, we use `write_database` from `make_melting_database.py` to generate the melting temperature database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_database = False\n",
    "if write_database:\n",
    "    start_n_bases = 6\n",
    "    end_n_bases = 9\n",
    "    duplex_umol_conc = [1,2]\n",
    "    salt_mol_conc = [0.1, 0.2, 0.5]\n",
    "    make_melting_database.write_database(start_n_bases, end_n_bases, duplex_umol_conc, salt_mol_conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the dataset as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melting_database\\\\6meres.csv', 'melting_database\\\\7meres.csv', 'melting_database\\\\8meres.csv', 'melting_database\\\\9meres.csv']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0</th>\n",
       "      <th>base_1</th>\n",
       "      <th>base_2</th>\n",
       "      <th>base_3</th>\n",
       "      <th>base_4</th>\n",
       "      <th>base_5</th>\n",
       "      <th>base_6</th>\n",
       "      <th>base_7</th>\n",
       "      <th>base_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               seq  s_mol  c_umol        T_m base_0 base_1 base_2 base_3  \\\n",
       "0           AAAAAA    0.1       1 -20.495855      A      A      A      A   \n",
       "1           AAAAAA    0.2       1 -17.536560      A      A      A      A   \n",
       "2           AAAAAA    0.5       1 -13.516521      A      A      A      A   \n",
       "3           AAAAAA    0.1       2 -17.837395      A      A      A      A   \n",
       "4           AAAAAA    0.2       2 -14.815124      A      A      A      A   \n",
       "...            ...    ...     ...        ...    ...    ...    ...    ...   \n",
       "2088955  GGGGGGGGG    0.2       1  47.545073      G      G      G      G   \n",
       "2088956  GGGGGGGGG    0.5       1  52.512866      G      G      G      G   \n",
       "2088957  GGGGGGGGG    0.1       2  46.070145      G      G      G      G   \n",
       "2088958  GGGGGGGGG    0.2       2  49.779454      G      G      G      G   \n",
       "2088959  GGGGGGGGG    0.5       2  54.817256      G      G      G      G   \n",
       "\n",
       "        base_4 base_5 base_6 base_7 base_8  \n",
       "0            A      A      0      0      0  \n",
       "1            A      A      0      0      0  \n",
       "2            A      A      0      0      0  \n",
       "3            A      A      0      0      0  \n",
       "4            A      A      0      0      0  \n",
       "...        ...    ...    ...    ...    ...  \n",
       "2088955      G      G      G      G      G  \n",
       "2088956      G      G      G      G      G  \n",
       "2088957      G      G      G      G      G  \n",
       "2088958      G      G      G      G      G  \n",
       "2088959      G      G      G      G      G  \n",
       "\n",
       "[2088960 rows x 13 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather the data from the various files and pool them into one dataframe\n",
    "database_dir = 'melting_database'\n",
    "datasets = os.listdir(database_dir)\n",
    "\n",
    "all_files = [os.path.join(database_dir, data) for data in datasets]\n",
    "print(all_files)\n",
    "df = pd.concat((pd.read_csv(f, index_col=0) for f in all_files), ignore_index=True)\n",
    "\n",
    "df\n",
    "#dataset = torch.utils.data.TensorDataset(x, T_m)\n",
    "#dataloader = torch.utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-hot encoded bases fed to a fully connected network\n",
    "\n",
    "The simplest model we will try is a fully connected network attached connected to one-hot encoded nucleotides. First of all, we perform the one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0_0</th>\n",
       "      <th>base_0_A</th>\n",
       "      <th>base_0_C</th>\n",
       "      <th>base_0_G</th>\n",
       "      <th>base_0_T</th>\n",
       "      <th>base_1_0</th>\n",
       "      <th>base_1_A</th>\n",
       "      <th>...</th>\n",
       "      <th>base_7_0</th>\n",
       "      <th>base_7_A</th>\n",
       "      <th>base_7_C</th>\n",
       "      <th>base_7_G</th>\n",
       "      <th>base_7_T</th>\n",
       "      <th>base_8_0</th>\n",
       "      <th>base_8_A</th>\n",
       "      <th>base_8_C</th>\n",
       "      <th>base_8_G</th>\n",
       "      <th>base_8_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         s_mol  c_umol        T_m  base_0_0  base_0_A  base_0_C  base_0_G  \\\n",
       "0          0.1       1 -20.495855         0         1         0         0   \n",
       "1          0.2       1 -17.536560         0         1         0         0   \n",
       "2          0.5       1 -13.516521         0         1         0         0   \n",
       "3          0.1       2 -17.837395         0         1         0         0   \n",
       "4          0.2       2 -14.815124         0         1         0         0   \n",
       "...        ...     ...        ...       ...       ...       ...       ...   \n",
       "2088955    0.2       1  47.545073         0         0         0         1   \n",
       "2088956    0.5       1  52.512866         0         0         0         1   \n",
       "2088957    0.1       2  46.070145         0         0         0         1   \n",
       "2088958    0.2       2  49.779454         0         0         0         1   \n",
       "2088959    0.5       2  54.817256         0         0         0         1   \n",
       "\n",
       "         base_0_T  base_1_0  base_1_A  ...  base_7_0  base_7_A  base_7_C  \\\n",
       "0               0         0         1  ...         1         0         0   \n",
       "1               0         0         1  ...         1         0         0   \n",
       "2               0         0         1  ...         1         0         0   \n",
       "3               0         0         1  ...         1         0         0   \n",
       "4               0         0         1  ...         1         0         0   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "2088955         0         0         0  ...         0         0         0   \n",
       "2088956         0         0         0  ...         0         0         0   \n",
       "2088957         0         0         0  ...         0         0         0   \n",
       "2088958         0         0         0  ...         0         0         0   \n",
       "2088959         0         0         0  ...         0         0         0   \n",
       "\n",
       "         base_7_G  base_7_T  base_8_0  base_8_A  base_8_C  base_8_G  base_8_T  \n",
       "0               0         0         1         0         0         0         0  \n",
       "1               0         0         1         0         0         0         0  \n",
       "2               0         0         1         0         0         0         0  \n",
       "3               0         0         1         0         0         0         0  \n",
       "4               0         0         1         0         0         0         0  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "2088955         1         0         0         0         0         1         0  \n",
       "2088956         1         0         0         0         0         1         0  \n",
       "2088957         1         0         0         0         0         1         0  \n",
       "2088958         1         0         0         0         0         1         0  \n",
       "2088959         1         0         0         0         0         1         0  \n",
       "\n",
       "[2088960 rows x 48 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_oh = df.copy()\n",
    "# We add (and then remove) a sequence with all missing bases to ensure that\n",
    "# every position has the same number of dummy columns. Not required for now,\n",
    "# but required for steps 3 and 4\n",
    "df_oh.loc[df.shape[0]] = df.shape[1] * [0]\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    target_col = 'base_' + str(i)\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df[target_col]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # generate the one-hot encoded columns and append them to the dataframe\n",
    "    new_cols = pd.get_dummies(df_oh[target_col], prefix = target_col)\n",
    "    df_oh = pd.concat([df_oh, new_cols], axis=1)\n",
    "    # remove original column from the dataframe\n",
    "    df_oh.drop(target_col, axis=1, inplace=True)\n",
    "    i += 1\n",
    "df_oh.drop('seq', axis=1, inplace=True)\n",
    "df_oh.drop(df_oh.shape[0]-1,inplace=True)\n",
    "df_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data between the training set and the validation set, and load it into dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1880064 instances\n",
      "Validation set has 208896 instances\n"
     ]
    }
   ],
   "source": [
    "# Cast the data into a dataset\n",
    "y = torch.Tensor(df.T_m, device='cuda')\n",
    "x = torch.Tensor(df_oh.drop('T_m', axis=1).values, device='cuda')\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x, y)\n",
    "\n",
    "train_length = int(len(dataset) * 0.9)\n",
    "valid_length = len(dataset) - train_length\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_length, valid_length], generator=torch.Generator(device='cuda'))\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_set)} instances')\n",
    "print(f'Validation set has {len(valid_set)} instances')\n",
    "\n",
    "# Create the loaders\n",
    "batch_size = 64\n",
    "training_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "validation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the model. In this part of the notebook we use a simple fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, input_size : int, n_hidden_layers : int, hidden_size : int):\n",
    "        super(FC, self).__init__()\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for n in range(self.n_hidden_layers):\n",
    "            x = F.relu(self.hidden_layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = FC(x.shape[1], 1, x.shape[1] * 3).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry on defining the loss and the optimizer. We use a standard MSE loss and an Adam optimizer with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop - taken from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop sweeps the data over multiple epochs. Once per epoch, we perform model validation by checking model performance on validation data, and save a copy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute '_UntypedStorage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ferdi\\Dropbox\\Programmazione\\DNAMelting\\notebook.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number, writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# We don't need gradients on to do reporting\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\ferdi\\Dropbox\\Programmazione\\DNAMelting\\notebook.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m last_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Here, we use enumerate(training_loader) instead of\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# iter(training_loader) so that we can track the batch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# index and do some intra-epoch reporting\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(training_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Every data instance is an input + label pair\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Zero your gradients for every batch!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:444\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:390\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1077\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1070\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:253\u001b[0m, in \u001b[0;36mreduce_tensor\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m    244\u001b[0m (device,\n\u001b[0;32m    245\u001b[0m  handle,\n\u001b[0;32m    246\u001b[0m  storage_size_bytes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m  event_handle,\n\u001b[0;32m    251\u001b[0m  event_sync_required) \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39m_share_cuda_()\n\u001b[0;32m    252\u001b[0m tensor_offset \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mstorage_offset()\n\u001b[1;32m--> 253\u001b[0m shared_cache[handle] \u001b[39m=\u001b[39m StorageWeakRef(storage)\n\u001b[0;32m    254\u001b[0m \u001b[39m# _backward_hooks purposely omitted here, see\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39m# Note [Don't serialize hooks]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39mreturn\u001b[39;00m (rebuild_cuda_tensor,\n\u001b[0;32m    257\u001b[0m         (\u001b[39mtype\u001b[39m(tensor),\n\u001b[0;32m    258\u001b[0m          tensor\u001b[39m.\u001b[39msize(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    270\u001b[0m          event_handle,\n\u001b[0;32m    271\u001b[0m          event_sync_required))\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:65\u001b[0m, in \u001b[0;36mSharedCache.__setitem__\u001b[1;34m(self, key, storage_ref)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdict\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key, storage_ref)\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfree_dead_references()\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:70\u001b[0m, in \u001b[0;36mSharedCache.free_dead_references\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m live \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m key, storage_ref \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mif\u001b[39;00m storage_ref\u001b[39m.\u001b[39;49mexpired():\n\u001b[0;32m     71\u001b[0m         \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     72\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py:35\u001b[0m, in \u001b[0;36mStorageWeakRef.expired\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpired\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mStorage\u001b[39m.\u001b[39;49m_expired(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcdata)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\torch\\storage.py:757\u001b[0m, in \u001b[0;36m_TypedStorage._expired\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_expired\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 757\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39meval\u001b[39;49m(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__module__\u001b[39;49m)\u001b[39m.\u001b[39;49m_UntypedStorage\u001b[39m.\u001b[39m_expired(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute '_UntypedStorage'"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the model to reproducing melting temperatures of sequences with length between 6 and 9, the MSE loss plateaus at 6.4, while the L1 loss gets to 1.7. Not bad, given the wide range of melting temperatures observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41,)\n",
      "(41,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ferdi\\Dropbox\\Programmazione\\DNAMelting\\notebook.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(model(inputs)[:inputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m,:]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mflatten()\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m,:]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ferdi/Dropbox/Programmazione/DNAMelting/notebook.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df_plot \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mpredicted\u001b[39;49m\u001b[39m'\u001b[39;49m : model(inputs)[:,\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()[:inputs\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]], \u001b[39m'\u001b[39;49m\u001b[39mcomputed\u001b[39;49m\u001b[39m'\u001b[39;49m : inputs\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()[\u001b[39m0\u001b[39;49m]})\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\ferdi\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(training_loader))\n",
    "#labels\n",
    "x = \n",
    "print(model(inputs)[:inputs.cpu().numpy()[0,:].shape[0],0].detach().numpy().flatten().shape)\n",
    "print(inputs.cpu().numpy()[0,:].shape)\n",
    "\n",
    "df_plot = pd.DataFrame({'predicted' : model(inputs)[:,0].detach().numpy()[:inputs.cpu().numpy().shape[0]], 'computed' : inputs.cpu().numpy()[0]})\n",
    "#df_plot.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(training_loader):\n",
    "    inputs, labels = data\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    break\n",
    "for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        #vlabels = torch.unsqueeze(vlabels, 1)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-hot encoded base-pair steps fed to a fully connected network\n",
    "First of all, we need to generate the columns containing the base-pair step of each sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0</th>\n",
       "      <th>base_1</th>\n",
       "      <th>base_2</th>\n",
       "      <th>base_3</th>\n",
       "      <th>base_4</th>\n",
       "      <th>base_5</th>\n",
       "      <th>...</th>\n",
       "      <th>base_7</th>\n",
       "      <th>base_8</th>\n",
       "      <th>base_01</th>\n",
       "      <th>base_12</th>\n",
       "      <th>base_23</th>\n",
       "      <th>base_34</th>\n",
       "      <th>base_45</th>\n",
       "      <th>base_56</th>\n",
       "      <th>base_67</th>\n",
       "      <th>base_78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAA</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>AA</td>\n",
       "      <td>A0</td>\n",
       "      <td>00</td>\n",
       "      <td>00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>GGGGGGGGG</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "      <td>GG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               seq  s_mol  c_umol        T_m base_0 base_1 base_2 base_3  \\\n",
       "0           AAAAAA    0.1       1 -20.495855      A      A      A      A   \n",
       "1           AAAAAA    0.2       1 -17.536560      A      A      A      A   \n",
       "2           AAAAAA    0.5       1 -13.516521      A      A      A      A   \n",
       "3           AAAAAA    0.1       2 -17.837395      A      A      A      A   \n",
       "4           AAAAAA    0.2       2 -14.815124      A      A      A      A   \n",
       "...            ...    ...     ...        ...    ...    ...    ...    ...   \n",
       "2088955  GGGGGGGGG    0.2       1  47.545073      G      G      G      G   \n",
       "2088956  GGGGGGGGG    0.5       1  52.512866      G      G      G      G   \n",
       "2088957  GGGGGGGGG    0.1       2  46.070145      G      G      G      G   \n",
       "2088958  GGGGGGGGG    0.2       2  49.779454      G      G      G      G   \n",
       "2088959  GGGGGGGGG    0.5       2  54.817256      G      G      G      G   \n",
       "\n",
       "        base_4 base_5  ... base_7 base_8 base_01 base_12 base_23 base_34  \\\n",
       "0            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "1            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "2            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "3            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "4            A      A  ...      0      0      AA      AA      AA      AA   \n",
       "...        ...    ...  ...    ...    ...     ...     ...     ...     ...   \n",
       "2088955      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088956      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088957      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088958      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "2088959      G      G  ...      G      G      GG      GG      GG      GG   \n",
       "\n",
       "        base_45 base_56 base_67 base_78  \n",
       "0            AA      A0      00      00  \n",
       "1            AA      A0      00      00  \n",
       "2            AA      A0      00      00  \n",
       "3            AA      A0      00      00  \n",
       "4            AA      A0      00      00  \n",
       "...         ...     ...     ...     ...  \n",
       "2088955      GG      GG      GG      GG  \n",
       "2088956      GG      GG      GG      GG  \n",
       "2088957      GG      GG      GG      GG  \n",
       "2088958      GG      GG      GG      GG  \n",
       "2088959      GG      GG      GG      GG  \n",
       "\n",
       "[2088960 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bps = df.copy()\n",
    "i = 0\n",
    "while True:\n",
    "    base = 'base_'\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df[base + str(i + 1)]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # concatenate two columns to make base-pair steps. The 'astype(str)' are necessary to deal with the number-like 0 values in df_bps.\n",
    "    df_bps[base + str(i) + str(i + 1)] = df[base + str(i)].astype(str) + df[base + str(i+1)].astype(str)\n",
    "    i += 1    \n",
    "df_bps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the one-hot encoded columns similarly to how we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_01_AA</th>\n",
       "      <th>base_01_AC</th>\n",
       "      <th>base_01_AG</th>\n",
       "      <th>base_01_AT</th>\n",
       "      <th>base_01_CA</th>\n",
       "      <th>base_01_CC</th>\n",
       "      <th>base_01_CG</th>\n",
       "      <th>...</th>\n",
       "      <th>base_78_G0</th>\n",
       "      <th>base_78_GA</th>\n",
       "      <th>base_78_GC</th>\n",
       "      <th>base_78_GG</th>\n",
       "      <th>base_78_GT</th>\n",
       "      <th>base_78_T0</th>\n",
       "      <th>base_78_TA</th>\n",
       "      <th>base_78_TC</th>\n",
       "      <th>base_78_TG</th>\n",
       "      <th>base_78_TT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         s_mol  c_umol        T_m  base_01_AA  base_01_AC  base_01_AG  \\\n",
       "0          0.1       1 -20.495855           1           0           0   \n",
       "1          0.2       1 -17.536560           1           0           0   \n",
       "2          0.5       1 -13.516521           1           0           0   \n",
       "3          0.1       2 -17.837395           1           0           0   \n",
       "4          0.2       2 -14.815124           1           0           0   \n",
       "...        ...     ...        ...         ...         ...         ...   \n",
       "2088955    0.2       1  47.545073           0           0           0   \n",
       "2088956    0.5       1  52.512866           0           0           0   \n",
       "2088957    0.1       2  46.070145           0           0           0   \n",
       "2088958    0.2       2  49.779454           0           0           0   \n",
       "2088959    0.5       2  54.817256           0           0           0   \n",
       "\n",
       "         base_01_AT  base_01_CA  base_01_CC  base_01_CG  ...  base_78_G0  \\\n",
       "0                 0           0           0           0  ...           0   \n",
       "1                 0           0           0           0  ...           0   \n",
       "2                 0           0           0           0  ...           0   \n",
       "3                 0           0           0           0  ...           0   \n",
       "4                 0           0           0           0  ...           0   \n",
       "...             ...         ...         ...         ...  ...         ...   \n",
       "2088955           0           0           0           0  ...           0   \n",
       "2088956           0           0           0           0  ...           0   \n",
       "2088957           0           0           0           0  ...           0   \n",
       "2088958           0           0           0           0  ...           0   \n",
       "2088959           0           0           0           0  ...           0   \n",
       "\n",
       "         base_78_GA  base_78_GC  base_78_GG  base_78_GT  base_78_T0  \\\n",
       "0                 0           0           0           0           0   \n",
       "1                 0           0           0           0           0   \n",
       "2                 0           0           0           0           0   \n",
       "3                 0           0           0           0           0   \n",
       "4                 0           0           0           0           0   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "2088955           0           0           1           0           0   \n",
       "2088956           0           0           1           0           0   \n",
       "2088957           0           0           1           0           0   \n",
       "2088958           0           0           1           0           0   \n",
       "2088959           0           0           1           0           0   \n",
       "\n",
       "         base_78_TA  base_78_TC  base_78_TG  base_78_TT  \n",
       "0                 0           0           0           0  \n",
       "1                 0           0           0           0  \n",
       "2                 0           0           0           0  \n",
       "3                 0           0           0           0  \n",
       "4                 0           0           0           0  \n",
       "...             ...         ...         ...         ...  \n",
       "2088955           0           0           0           0  \n",
       "2088956           0           0           0           0  \n",
       "2088957           0           0           0           0  \n",
       "2088958           0           0           0           0  \n",
       "2088959           0           0           0           0  \n",
       "\n",
       "[2088960 rows x 145 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bps_oh = df_bps.copy()\n",
    "i = 0\n",
    "while True:\n",
    "    base = 'base_'\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df_bps[base + str(i + 1)]\n",
    "    except KeyError:\n",
    "        break\n",
    "    # get new one-hot encoded columns\n",
    "    target_col = base + str(i) + str(i + 1)\n",
    "    new_cols = pd.get_dummies(df_bps[target_col], prefix = target_col)\n",
    "    # concatenate them to the dataframe and drop the original columns\n",
    "    df_bps_oh = pd.concat([df_bps_oh, new_cols], axis=1)\n",
    "    df_bps_oh.drop([target_col, base + str(i)], axis=1, inplace=True)\n",
    "    i += 1\n",
    "df_bps_oh.drop(['seq', base + str(i)], axis=1, inplace=True)\n",
    "df_bps_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the variables are encoded, we can load the data into dataloader objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1880064 instances\n",
      "Validation set has 208896 instances\n"
     ]
    }
   ],
   "source": [
    "# Cast the data into a dataset\n",
    "y_bps = torch.Tensor(df_bps.T_m)\n",
    "x_bps = torch.Tensor(df_bps_oh.drop('T_m', axis=1).values)\n",
    "\n",
    "dataset_bps = TensorDataset(x_bps, y_bps)\n",
    "\n",
    "train_length_bps = int(len(dataset_bps) * 0.9)\n",
    "valid_length_bps = len(dataset_bps) - train_length_bps\n",
    "train_set_bps, valid_set_bps = torch.utils.data.random_split(dataset_bps, [train_length_bps, valid_length_bps])\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_set_bps)} instances')\n",
    "print(f'Validation set has {len(valid_set_bps)} instances')\n",
    "\n",
    "# Create the loaders\n",
    "batch_size_bps = 64\n",
    "training_loader_bps = DataLoader(train_set_bps, batch_size=batch_size_bps, shuffle=False, num_workers=2)\n",
    "validation_loader_bps = DataLoader(valid_set_bps, batch_size=batch_size_bps, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model structure, loss and optimizer are the same as in the previous case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bps = FC(x_bps.shape[1], 1, x_bps.shape[1] * 2)\n",
    "#loss_fn_bps = nn.MSELoss()\n",
    "loss_fn_bps = nn.L1Loss()\n",
    "optimizer_bps = torch.optim.Adam(model_bps.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rewrite a training function, `train_one_epoch_bps` just renaming some of the variable it uses from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_bps(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader_bps):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_bps.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_bps(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "\n",
    "        loss = loss_fn_bps(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_bps.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader_bps) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 3.1822671583890916\n",
      "  batch 2000 loss: 2.153823205471039\n",
      "  batch 3000 loss: 2.0897410534620287\n",
      "  batch 4000 loss: 2.0579217829704284\n",
      "  batch 5000 loss: 2.028912232875824\n",
      "  batch 6000 loss: 1.9860537939071656\n",
      "  batch 7000 loss: 1.9585571776628494\n",
      "  batch 8000 loss: 1.9517711392641068\n",
      "  batch 9000 loss: 1.9217170761823654\n",
      "  batch 10000 loss: 1.9066286067962646\n",
      "  batch 11000 loss: 1.87270246386528\n",
      "  batch 12000 loss: 1.8746917355060577\n",
      "  batch 13000 loss: 1.8473758735656738\n",
      "  batch 14000 loss: 1.8387831867933273\n",
      "  batch 15000 loss: 1.7919547945261\n",
      "  batch 16000 loss: 1.7881101115942002\n",
      "  batch 17000 loss: 1.76903054356575\n",
      "  batch 18000 loss: 1.7424039701223373\n",
      "  batch 19000 loss: 1.7010922545194627\n",
      "  batch 20000 loss: 1.6498824585676193\n",
      "  batch 21000 loss: 1.6243142762184144\n",
      "  batch 22000 loss: 1.5981925272941588\n",
      "  batch 23000 loss: 1.576833158493042\n",
      "  batch 24000 loss: 1.5528614037036896\n",
      "  batch 25000 loss: 1.5299864151477813\n",
      "  batch 26000 loss: 1.5119411797523499\n",
      "  batch 27000 loss: 1.4898853675723076\n",
      "  batch 28000 loss: 1.484999808907509\n",
      "  batch 29000 loss: 1.454005694746971\n",
      "LOSS train 1.454005694746971 valid 0.39712175726890564\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.4227104406356812\n",
      "  batch 2000 loss: 1.4222153540849685\n",
      "  batch 3000 loss: 1.3892530454993248\n",
      "  batch 4000 loss: 1.379512231528759\n",
      "  batch 5000 loss: 1.361702245235443\n",
      "  batch 6000 loss: 1.3462938575148582\n",
      "  batch 7000 loss: 1.3227297152876853\n",
      "  batch 8000 loss: 1.3087049268484117\n",
      "  batch 9000 loss: 1.2910269519090654\n",
      "  batch 10000 loss: 1.2778465554714202\n",
      "  batch 11000 loss: 1.2640210304260253\n",
      "  batch 12000 loss: 1.2378400915265084\n",
      "  batch 13000 loss: 1.2370607565641403\n",
      "  batch 14000 loss: 1.2116992347836495\n",
      "  batch 15000 loss: 1.1854385338425637\n",
      "  batch 16000 loss: 1.17639699447155\n",
      "  batch 17000 loss: 1.1647854313254355\n",
      "  batch 18000 loss: 1.1422402128577231\n",
      "  batch 19000 loss: 1.1336035964488984\n",
      "  batch 20000 loss: 1.1282056271433831\n",
      "  batch 21000 loss: 1.1021875467300415\n",
      "  batch 22000 loss: 1.0930442311763764\n",
      "  batch 23000 loss: 1.080316332578659\n",
      "  batch 24000 loss: 1.0548486093878746\n",
      "  batch 25000 loss: 1.052894099712372\n",
      "  batch 26000 loss: 1.0398816530108452\n",
      "  batch 27000 loss: 1.0235421991944313\n",
      "  batch 28000 loss: 1.013130248606205\n",
      "  batch 29000 loss: 0.99611376953125\n",
      "LOSS train 0.99611376953125 valid 0.34481868147850037\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.9820279369354248\n",
      "  batch 2000 loss: 0.9676989620327949\n",
      "  batch 3000 loss: 0.9550048432946205\n",
      "  batch 4000 loss: 0.958834297478199\n",
      "  batch 5000 loss: 0.9401376486420632\n",
      "  batch 6000 loss: 0.926300121307373\n",
      "  batch 7000 loss: 0.923243819117546\n",
      "  batch 8000 loss: 0.9083717643618584\n",
      "  batch 9000 loss: 0.9004437889456749\n",
      "  batch 10000 loss: 0.8923355716466904\n",
      "  batch 11000 loss: 0.8798088073134422\n",
      "  batch 12000 loss: 0.8720185289978981\n",
      "  batch 13000 loss: 0.8673008415699005\n",
      "  batch 14000 loss: 0.8561330177187919\n",
      "  batch 15000 loss: 0.8465658923983574\n",
      "  batch 16000 loss: 0.843646334528923\n",
      "  batch 17000 loss: 0.8351986977458\n",
      "  batch 18000 loss: 0.8236976844668389\n",
      "  batch 19000 loss: 0.8234458750486374\n",
      "  batch 20000 loss: 0.8161131251454353\n",
      "  batch 21000 loss: 0.8077764549255371\n",
      "  batch 22000 loss: 0.7941264694929123\n",
      "  batch 23000 loss: 0.7995709733366966\n",
      "  batch 24000 loss: 0.7890362235307693\n",
      "  batch 25000 loss: 0.7834043477773667\n",
      "  batch 26000 loss: 0.778829781293869\n",
      "  batch 27000 loss: 0.7659688260257244\n",
      "  batch 28000 loss: 0.7713310089409351\n",
      "  batch 29000 loss: 0.7613974587023258\n",
      "LOSS train 0.7613974587023258 valid 0.23079891502857208\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.7485619547367096\n",
      "  batch 2000 loss: 0.7529048922657967\n",
      "  batch 3000 loss: 0.7516254785358906\n",
      "  batch 4000 loss: 0.741264574766159\n",
      "  batch 5000 loss: 0.7400248896479606\n",
      "  batch 6000 loss: 0.7354900346398353\n",
      "  batch 7000 loss: 0.7261352847516537\n",
      "  batch 8000 loss: 0.7301087932884693\n",
      "  batch 9000 loss: 0.7304227376878262\n",
      "  batch 10000 loss: 0.7258908544480801\n",
      "  batch 11000 loss: 0.7207600066065788\n",
      "  batch 12000 loss: 0.7183490107059479\n",
      "  batch 13000 loss: 0.7161964261829853\n",
      "  batch 14000 loss: 0.7156886259019375\n",
      "  batch 15000 loss: 0.7160613964498043\n",
      "  batch 16000 loss: 0.7043933631181717\n",
      "  batch 17000 loss: 0.7036432383358479\n",
      "  batch 18000 loss: 0.7019574269354344\n",
      "  batch 19000 loss: 0.7020872903466224\n",
      "  batch 20000 loss: 0.7037575587332249\n",
      "  batch 21000 loss: 0.7022703278958797\n",
      "  batch 22000 loss: 0.6960477756857872\n",
      "  batch 23000 loss: 0.6886681890487671\n",
      "  batch 24000 loss: 0.6922881626188755\n",
      "  batch 25000 loss: 0.6934771855771542\n",
      "  batch 26000 loss: 0.6938882797956467\n",
      "  batch 27000 loss: 0.6914806348085404\n",
      "  batch 28000 loss: 0.6890766705274582\n",
      "  batch 29000 loss: 0.6889220975041389\n",
      "LOSS train 0.6889220975041389 valid 0.21476028859615326\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.6860545220971107\n",
      "  batch 2000 loss: 0.6863253947198391\n",
      "  batch 3000 loss: 0.6855304327011108\n",
      "  batch 4000 loss: 0.6856883489787579\n",
      "  batch 5000 loss: 0.6759277283251286\n",
      "  batch 6000 loss: 0.6748142503201962\n",
      "  batch 7000 loss: 0.6741906354725361\n",
      "  batch 8000 loss: 0.6751108303666115\n",
      "  batch 9000 loss: 0.679197796702385\n",
      "  batch 10000 loss: 0.6806329899728298\n",
      "  batch 11000 loss: 0.6781573359966278\n",
      "  batch 12000 loss: 0.6765035853385926\n",
      "  batch 13000 loss: 0.6793133171498775\n",
      "  batch 14000 loss: 0.6772563578486442\n",
      "  batch 15000 loss: 0.6699636433124542\n",
      "  batch 16000 loss: 0.6713634942770004\n",
      "  batch 17000 loss: 0.6770281980931759\n",
      "  batch 18000 loss: 0.6721201097667218\n",
      "  batch 19000 loss: 0.6749001289606095\n",
      "  batch 20000 loss: 0.6724877127707004\n",
      "  batch 21000 loss: 0.6702113339602948\n",
      "  batch 22000 loss: 0.6729177185893058\n",
      "  batch 23000 loss: 0.6678553042709827\n",
      "  batch 24000 loss: 0.6675422851145267\n",
      "  batch 25000 loss: 0.6674430347681045\n",
      "  batch 26000 loss: 0.6750765001475811\n",
      "  batch 27000 loss: 0.6695905621945858\n",
      "  batch 28000 loss: 0.6698973070979118\n",
      "  batch 29000 loss: 0.6662088871598244\n",
      "LOSS train 0.6662088871598244 valid 0.16366106271743774\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_bps.train(True)\n",
    "    avg_loss = train_one_epoch_bps(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_bps.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader_bps):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_bps(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_bps(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_bps = 'model_bps_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_bps.state_dict(), model_path_bps)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the MSE loss decreases all the way to `xxx` and the L1 loss gets to `xxx`. Let's see if we can get even lower than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.9623],\n",
       "        [23.6786],\n",
       "        [21.2388],\n",
       "        [25.3707],\n",
       "        [16.7135],\n",
       "        [35.6994],\n",
       "        [17.4662],\n",
       "        [20.6185],\n",
       "        [20.8223],\n",
       "        [40.8590],\n",
       "        [28.3726],\n",
       "        [31.9507],\n",
       "        [28.1155],\n",
       "        [26.7604],\n",
       "        [45.5279],\n",
       "        [28.2286],\n",
       "        [28.4248],\n",
       "        [39.4575],\n",
       "        [35.6994],\n",
       "        [45.3415],\n",
       "        [26.1465],\n",
       "        [35.6994],\n",
       "        [20.9257],\n",
       "        [33.5044],\n",
       "        [16.7135],\n",
       "        [28.4644],\n",
       "        [18.7368],\n",
       "        [26.2195],\n",
       "        [20.7873],\n",
       "        [39.5623],\n",
       "        [31.9306],\n",
       "        [35.6994],\n",
       "        [16.7135],\n",
       "        [19.2009],\n",
       "        [20.6243],\n",
       "        [22.1057],\n",
       "        [36.8177],\n",
       "        [18.7416],\n",
       "        [28.3219],\n",
       "        [31.0323],\n",
       "        [28.1812],\n",
       "        [30.9107],\n",
       "        [16.7135],\n",
       "        [32.6798],\n",
       "        [27.5582],\n",
       "        [35.7365],\n",
       "        [45.0228],\n",
       "        [16.7135],\n",
       "        [32.2754],\n",
       "        [21.8763],\n",
       "        [35.6994],\n",
       "        [30.7928],\n",
       "        [35.6994],\n",
       "        [35.6994],\n",
       "        [44.5405],\n",
       "        [41.5874],\n",
       "        [35.6994],\n",
       "        [28.8313],\n",
       "        [23.8438],\n",
       "        [27.6061],\n",
       "        [41.1817],\n",
       "        [27.6908],\n",
       "        [35.6994],\n",
       "        [32.9058]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(training_loader))\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoded bases fed to a recurring neural network and fully connected network\n",
    "The DNA sequence can be expressed as a string, so it feels natural to try and tackle this problem with a recurring neural network. Let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2088960, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_mol</th>\n",
       "      <th>c_umol</th>\n",
       "      <th>T_m</th>\n",
       "      <th>base_0_0</th>\n",
       "      <th>base_0_A</th>\n",
       "      <th>base_0_C</th>\n",
       "      <th>base_0_G</th>\n",
       "      <th>base_0_T</th>\n",
       "      <th>base_1_0</th>\n",
       "      <th>base_1_A</th>\n",
       "      <th>...</th>\n",
       "      <th>base_7_0</th>\n",
       "      <th>base_7_A</th>\n",
       "      <th>base_7_C</th>\n",
       "      <th>base_7_G</th>\n",
       "      <th>base_7_T</th>\n",
       "      <th>base_8_0</th>\n",
       "      <th>base_8_A</th>\n",
       "      <th>base_8_C</th>\n",
       "      <th>base_8_G</th>\n",
       "      <th>base_8_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.495855</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-17.536560</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-13.516521</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-17.837395</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-14.815124</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088955</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.545073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088956</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.512866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088957</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.070145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088958</th>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.779454</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088959</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.817256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2088960 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         s_mol  c_umol        T_m  base_0_0  base_0_A  base_0_C  base_0_G  \\\n",
       "0          0.1     1.0 -20.495855         0         1         0         0   \n",
       "1          0.2     1.0 -17.536560         0         1         0         0   \n",
       "2          0.5     1.0 -13.516521         0         1         0         0   \n",
       "3          0.1     2.0 -17.837395         0         1         0         0   \n",
       "4          0.2     2.0 -14.815124         0         1         0         0   \n",
       "...        ...     ...        ...       ...       ...       ...       ...   \n",
       "2088955    0.2     1.0  47.545073         0         0         0         1   \n",
       "2088956    0.5     1.0  52.512866         0         0         0         1   \n",
       "2088957    0.1     2.0  46.070145         0         0         0         1   \n",
       "2088958    0.2     2.0  49.779454         0         0         0         1   \n",
       "2088959    0.5     2.0  54.817256         0         0         0         1   \n",
       "\n",
       "         base_0_T  base_1_0  base_1_A  ...  base_7_0  base_7_A  base_7_C  \\\n",
       "0               0         0         1  ...         1         0         0   \n",
       "1               0         0         1  ...         1         0         0   \n",
       "2               0         0         1  ...         1         0         0   \n",
       "3               0         0         1  ...         1         0         0   \n",
       "4               0         0         1  ...         1         0         0   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "2088955         0         0         0  ...         0         0         0   \n",
       "2088956         0         0         0  ...         0         0         0   \n",
       "2088957         0         0         0  ...         0         0         0   \n",
       "2088958         0         0         0  ...         0         0         0   \n",
       "2088959         0         0         0  ...         0         0         0   \n",
       "\n",
       "         base_7_G  base_7_T  base_8_0  base_8_A  base_8_C  base_8_G  base_8_T  \n",
       "0               0         0         1         0         0         0         0  \n",
       "1               0         0         1         0         0         0         0  \n",
       "2               0         0         1         0         0         0         0  \n",
       "3               0         0         1         0         0         0         0  \n",
       "4               0         0         1         0         0         0         0  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "2088955         1         0         0         0         0         1         0  \n",
       "2088956         1         0         0         0         0         1         0  \n",
       "2088957         1         0         0         0         0         1         0  \n",
       "2088958         1         0         0         0         0         1         0  \n",
       "2088959         1         0         0         0         0         1         0  \n",
       "\n",
       "[2088960 rows x 48 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# We can't reuse df_oh because that does not encode the possibiilty of having base 0 in the first few positions.\n",
    "# This means that the one-hot encoded vectors don't have all the same dimensions.\n",
    "df_oh_f = df.copy()\n",
    "df_temp = df.copy()\n",
    "df_temp.loc[df.shape[0]] = df_temp.shape[1] * [0]\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    target_col = 'base_' + str(i)\n",
    "    # get out of the loop when we reach the last 'base_n' column\n",
    "    try:\n",
    "        df_temp[target_col]\n",
    "    except KeyError:\n",
    "        break\n",
    "    new_cols = pd.get_dummies(df_temp[target_col], prefix = target_col)\n",
    "    df_oh_f = pd.concat([df_oh_f, new_cols], axis=1)\n",
    "    df_oh_f.drop(target_col, axis=1, inplace=True)\n",
    "    i += 1\n",
    "df_oh_f.drop('seq', axis=1, inplace=True)\n",
    "df_oh_f.drop(df_oh_f.shape[0]-1,inplace=True)\n",
    "print(df_oh_f.shape)\n",
    "df_oh_f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did in the previous examples, we parse the dataset into `DataLoader` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1880064 instances\n",
      "Validation set has 208896 instances\n"
     ]
    }
   ],
   "source": [
    "# Cast the data into a dataset\n",
    "y_rnn = torch.Tensor(df_oh_f.T_m)\n",
    "x_rnn = torch.Tensor(df_oh_f.drop('T_m', axis=1).values)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(x_rnn, y_rnn)\n",
    "\n",
    "train_length = int(len(dataset) * 0.9)\n",
    "valid_length = len(dataset) - train_length\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [train_length, valid_length])\n",
    "\n",
    "# Report split sizes\n",
    "print(f'Training set has {len(train_set)} instances')\n",
    "print(f'Validation set has {len(valid_set)} instances')\n",
    "\n",
    "# Create the loaders\n",
    "batch_size = 16\n",
    "training_loader_rnn = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "validation_loader_rnn = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the dataloaders from point 1 above, so there is no need to define new dataloaders here.\n",
    "\n",
    "On the other hand, we used a flattened one-hot encoding in both the models above, but in order to use the `nn.RNN` module we will need to store the one-hot encoded variables into a tensor with shape `(batch_size, seq_len, n_bases)`. The other two variables (duplex and salt concentrations) can be fed as they are to the fully-connected network. Let's write a function to convert the one-hot variables into a tensor, to be called from within the `Model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bases = 5 # 4 possible letters among A, T, C, G, and 0 for no base.\n",
    "def get_conc_features(x):\n",
    "    '''\n",
    "    Given a batch of data x, return:\n",
    "    1. a tensor of shape (batch_size, 2) containing the concentrations, and\n",
    "    2. a tensor of shape (batch_size, seq_len, n_bases) containing the one-hot encoded sequence variables\n",
    "    '''\n",
    "    concs = x[:,:2]\n",
    "    oh_flat = x[:, 2:]\n",
    "    seq_len = df.shape[1] - 4# this uses the unprocessed dataframe read from file\n",
    "    \n",
    "    oh = torch.reshape(oh_flat, (x.shape[0],seq_len, n_bases))\n",
    "\n",
    "    return concs, oh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the model that we're going to use: a simple recurring neural network, whose output is fed to a fully connected network together with duplex and salt concentrations. We also define loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, seq_len):\n",
    "        super(RNNFC, self).__init__()\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        n_conc = 2\n",
    "        fc_hidden_dim = hidden_dim * seq_len + n_conc\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        # using relu nonlinearity to prevent vanishing gradient problems\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, batch_first=True, nonlinearity='relu')   \n",
    "        # Fully connected layers, receiving the output of rnn and the two concentrations\n",
    "        self.fc1 = nn.Linear(fc_hidden_dim, fc_hidden_dim)\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Split the input tensor between concentrations and one-hot encoded variables\n",
    "        concs, features = get_conc_features(x)\n",
    "\n",
    "        # Passing features into the rnn\n",
    "        # no need to pass a hidden state as it will be initialized to zero by default\n",
    "        out, hidden = self.rnn(features)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(batch_size, -1)\n",
    "        \n",
    "        # concatenating the outputs to the concentrations and feeding them to the\n",
    "        # fully connected layers\n",
    "        out = torch.cat((concs, out), axis=1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "seq_len = df.shape[1] - 4\n",
    "model_rnn = RNNFC(n_bases, 3, seq_len)\n",
    "#loss_fn_rnn = nn.MSELoss()\n",
    "loss_fn_rnn = nn.L1Loss()\n",
    "optimizer_rnn = torch.optim.Adam(model_rnn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also rewrite the training function, `train_one_epoch_rnn`, similarly to how we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_rnn(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_rnn.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_rnn(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        loss = loss_fn_rnn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 7.492772699594497\n",
      "  batch 2000 loss: 2.0802864763736726\n",
      "  batch 3000 loss: 1.8112838703393936\n",
      "  batch 4000 loss: 1.7793192170858383\n",
      "  batch 5000 loss: 1.7654063835144043\n",
      "  batch 6000 loss: 1.7470459654331207\n",
      "  batch 7000 loss: 1.7492566611766815\n",
      "  batch 8000 loss: 1.7193593571186065\n",
      "  batch 9000 loss: 1.7160402928590774\n",
      "  batch 10000 loss: 1.7246229212284088\n",
      "  batch 11000 loss: 1.7057601486444474\n",
      "  batch 12000 loss: 1.7130340725183486\n",
      "  batch 13000 loss: 1.7112231035232544\n",
      "  batch 14000 loss: 1.6990134575366973\n",
      "  batch 15000 loss: 1.710004292488098\n",
      "  batch 16000 loss: 1.7028482646942138\n",
      "  batch 17000 loss: 1.714467954158783\n",
      "  batch 18000 loss: 1.698732457280159\n",
      "  batch 19000 loss: 1.6976447479724883\n",
      "  batch 20000 loss: 1.6979927409887314\n",
      "  batch 21000 loss: 1.701554960012436\n",
      "  batch 22000 loss: 1.6780445975065232\n",
      "  batch 23000 loss: 1.6922220222949982\n",
      "  batch 24000 loss: 1.6896389483213425\n",
      "  batch 25000 loss: 1.6833447102308274\n",
      "  batch 26000 loss: 1.6757542066574096\n",
      "  batch 27000 loss: 1.6665161814689635\n",
      "  batch 28000 loss: 1.656001167178154\n",
      "  batch 29000 loss: 1.6389263937473297\n",
      "LOSS train 1.6389263937473297 valid 1.6747101545333862\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.6378118487596511\n",
      "  batch 2000 loss: 1.6108975203037261\n",
      "  batch 3000 loss: 1.591179164648056\n",
      "  batch 4000 loss: 1.58724105989933\n",
      "  batch 5000 loss: 1.559635186433792\n",
      "  batch 6000 loss: 1.5278038835525514\n",
      "  batch 7000 loss: 1.5173069583177567\n",
      "  batch 8000 loss: 1.4947410210371017\n",
      "  batch 9000 loss: 1.487497323513031\n",
      "  batch 10000 loss: 1.478277498304844\n",
      "  batch 11000 loss: 1.4562022368907928\n",
      "  batch 12000 loss: 1.4557325941324235\n",
      "  batch 13000 loss: 1.4559738085269929\n",
      "  batch 14000 loss: 1.4457512204647065\n",
      "  batch 15000 loss: 1.443168189883232\n",
      "  batch 16000 loss: 1.440262088060379\n",
      "  batch 17000 loss: 1.4463150464296342\n",
      "  batch 18000 loss: 1.4360177581310272\n",
      "  batch 19000 loss: 1.4274500547647475\n",
      "  batch 20000 loss: 1.43050293302536\n",
      "  batch 21000 loss: 1.422824399113655\n",
      "  batch 22000 loss: 1.4136477833390235\n",
      "  batch 23000 loss: 1.426337948679924\n",
      "  batch 24000 loss: 1.4241564359664918\n",
      "  batch 25000 loss: 1.4215624033212662\n",
      "  batch 26000 loss: 1.406972977757454\n",
      "  batch 27000 loss: 1.4035576291084289\n",
      "  batch 28000 loss: 1.4070412949323654\n",
      "  batch 29000 loss: 1.3989083794355393\n",
      "LOSS train 1.3989083794355393 valid 1.4019200801849365\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.3925904653072356\n",
      "  batch 2000 loss: 1.385908946454525\n",
      "  batch 3000 loss: 1.3835245643854142\n",
      "  batch 4000 loss: 1.3823899632692338\n",
      "  batch 5000 loss: 1.3821244655847549\n",
      "  batch 6000 loss: 1.3674148896336555\n",
      "  batch 7000 loss: 1.3755954107046127\n",
      "  batch 8000 loss: 1.370455971121788\n",
      "  batch 9000 loss: 1.367669685959816\n",
      "  batch 10000 loss: 1.365248032271862\n",
      "  batch 11000 loss: 1.3577745334506035\n",
      "  batch 12000 loss: 1.3609927446842194\n",
      "  batch 13000 loss: 1.3597296370267868\n",
      "  batch 14000 loss: 1.353469326555729\n",
      "  batch 15000 loss: 1.352021965444088\n",
      "  batch 16000 loss: 1.3474642517566682\n",
      "  batch 17000 loss: 1.3536288356781006\n",
      "  batch 18000 loss: 1.3409810274839402\n",
      "  batch 19000 loss: 1.3358345950245858\n",
      "  batch 20000 loss: 1.3364141263365745\n",
      "  batch 21000 loss: 1.3358321335911751\n",
      "  batch 22000 loss: 1.3292484146356582\n",
      "  batch 23000 loss: 1.3300870740413666\n",
      "  batch 24000 loss: 1.3350303232073784\n",
      "  batch 25000 loss: 1.3343413256406784\n",
      "  batch 26000 loss: 1.3148217994570732\n",
      "  batch 27000 loss: 1.3147154803872108\n",
      "  batch 28000 loss: 1.3216302575469017\n",
      "  batch 29000 loss: 1.3148945623636246\n",
      "LOSS train 1.3148945623636246 valid 1.2943134307861328\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.3086320335268975\n",
      "  batch 2000 loss: 1.3063395713567734\n",
      "  batch 3000 loss: 1.299724125802517\n",
      "  batch 4000 loss: 1.30671506357193\n",
      "  batch 5000 loss: 1.3007688264846802\n",
      "  batch 6000 loss: 1.2923736047744752\n",
      "  batch 7000 loss: 1.297629415690899\n",
      "  batch 8000 loss: 1.2912319979667664\n",
      "  batch 9000 loss: 1.286896836578846\n",
      "  batch 10000 loss: 1.2949875624775886\n",
      "  batch 11000 loss: 1.2809617891311647\n",
      "  batch 12000 loss: 1.2862947801947593\n",
      "  batch 13000 loss: 1.2890913624167442\n",
      "  batch 14000 loss: 1.2835347802639008\n",
      "  batch 15000 loss: 1.282411398768425\n",
      "  batch 16000 loss: 1.2762737784981728\n",
      "  batch 17000 loss: 1.2893643874526024\n",
      "  batch 18000 loss: 1.2777044768929482\n",
      "  batch 19000 loss: 1.2744084517359733\n",
      "  batch 20000 loss: 1.2760281804800033\n",
      "  batch 21000 loss: 1.275322185099125\n",
      "  batch 22000 loss: 1.2665627494454383\n",
      "  batch 23000 loss: 1.2750547000169754\n",
      "  batch 24000 loss: 1.2849069728255271\n",
      "  batch 25000 loss: 1.2749566718935967\n",
      "  batch 26000 loss: 1.2572960386276244\n",
      "  batch 27000 loss: 1.2573962265253067\n",
      "  batch 28000 loss: 1.2657836999297143\n",
      "  batch 29000 loss: 1.2662656566500663\n",
      "LOSS train 1.2662656566500663 valid 1.2498289346694946\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.2604046738743782\n",
      "  batch 2000 loss: 1.2540417416095733\n",
      "  batch 3000 loss: 1.2493210060596467\n",
      "  batch 4000 loss: 1.2581284403204918\n",
      "  batch 5000 loss: 1.2486794610619545\n",
      "  batch 6000 loss: 1.2446080939769746\n",
      "  batch 7000 loss: 1.2501584864258766\n",
      "  batch 8000 loss: 1.2441867891550065\n",
      "  batch 9000 loss: 1.24050843667984\n",
      "  batch 10000 loss: 1.2464312189817428\n",
      "  batch 11000 loss: 1.2321405934095382\n",
      "  batch 12000 loss: 1.2361775122880936\n",
      "  batch 13000 loss: 1.2377874745726585\n",
      "  batch 14000 loss: 1.2257024324536323\n",
      "  batch 15000 loss: 1.2248676005005836\n",
      "  batch 16000 loss: 1.2193461582660674\n",
      "  batch 17000 loss: 1.2251396206617355\n",
      "  batch 18000 loss: 1.2100184116959571\n",
      "  batch 19000 loss: 1.2011442587971688\n",
      "  batch 20000 loss: 1.1928424012064933\n",
      "  batch 21000 loss: 1.1833948219418526\n",
      "  batch 22000 loss: 1.16991263961792\n",
      "  batch 23000 loss: 1.1631610271930695\n",
      "  batch 24000 loss: 1.1627067641615867\n",
      "  batch 25000 loss: 1.1442748821377755\n",
      "  batch 26000 loss: 1.1246840067505837\n",
      "  batch 27000 loss: 1.110897354185581\n",
      "  batch 28000 loss: 1.1045472210645675\n",
      "  batch 29000 loss: 1.0931245613694192\n",
      "LOSS train 1.0931245613694192 valid 1.1231961250305176\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_rnn.train(True)\n",
    "    avg_loss = train_one_epoch_rnn(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_rnn.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_rnn(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_rnn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_rnn = 'model_rnn_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_rnn.state_dict(), model_path_rnn)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One-hot encoded bases fed to a recurring neural network and fully connected network\n",
    "\n",
    "Finally, we try replacing the RNN layer with a LSTM layer.\n",
    "\n",
    "To do so, we can keep the same loss and optimizer, and even reuse the `Model` object we used for the previous model: we just have to redefine its `rnn` layer to use a `LTSM` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = RNNFC(n_bases, 3, seq_len)\n",
    "model_lstm.rnn = nn.LSTM(n_bases, 3, batch_first=True)\n",
    "#loss_fn_lstm = nn.MSELoss()\n",
    "loss_fn_lstm = nn.L1Loss()\n",
    "optimizer_lstm = torch.optim.Adam(model_lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_lstm(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader_rnn):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer_lstm.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model_lstm(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        \n",
    "        loss = loss_fn_lstm(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader_rnn) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run the training loop on this model too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.15579303295910357\n",
      "  batch 2000 loss: 0.15416647762805225\n",
      "  batch 3000 loss: 0.15281109505146742\n",
      "  batch 4000 loss: 0.15363422441482544\n",
      "  batch 5000 loss: 0.15695616906136275\n",
      "  batch 6000 loss: 0.152926789291203\n",
      "  batch 7000 loss: 0.152254264049232\n",
      "  batch 8000 loss: 0.15166468101739883\n",
      "  batch 9000 loss: 0.15027427373081445\n",
      "  batch 10000 loss: 0.1522151111662388\n",
      "  batch 11000 loss: 0.15276156885176898\n",
      "  batch 12000 loss: 0.15270956530421972\n",
      "  batch 13000 loss: 0.14585737206041813\n",
      "  batch 14000 loss: 0.1491754691451788\n",
      "  batch 15000 loss: 0.14894684515893458\n",
      "  batch 16000 loss: 0.15191317104548216\n",
      "  batch 17000 loss: 0.1492836202606559\n",
      "  batch 18000 loss: 0.1482049672305584\n",
      "  batch 19000 loss: 0.1518049224987626\n",
      "  batch 20000 loss: 0.14886083249002696\n",
      "  batch 21000 loss: 0.1440218227878213\n",
      "  batch 22000 loss: 0.14998532424122096\n",
      "  batch 23000 loss: 0.1474542141929269\n",
      "  batch 24000 loss: 0.14499065185338258\n",
      "  batch 25000 loss: 0.14775263911485673\n",
      "  batch 26000 loss: 0.1429321006461978\n",
      "  batch 27000 loss: 0.14452025043964387\n",
      "  batch 28000 loss: 0.14446580070257187\n",
      "  batch 29000 loss: 0.14546703298389912\n",
      "  batch 30000 loss: 0.14556537117809057\n",
      "  batch 31000 loss: 0.1441505895629525\n",
      "  batch 32000 loss: 0.14834786976873876\n",
      "  batch 33000 loss: 0.1472957706898451\n",
      "  batch 34000 loss: 0.1470975718125701\n",
      "  batch 35000 loss: 0.14525647380203008\n",
      "  batch 36000 loss: 0.1437315613552928\n",
      "  batch 37000 loss: 0.14475250686705113\n",
      "  batch 38000 loss: 0.14409766291826964\n",
      "  batch 39000 loss: 0.14111786103993654\n",
      "  batch 40000 loss: 0.14399773782491684\n",
      "  batch 41000 loss: 0.14348567417263985\n",
      "  batch 42000 loss: 0.14231854835152627\n",
      "  batch 43000 loss: 0.14247565519064664\n",
      "  batch 44000 loss: 0.14301691753417253\n",
      "  batch 45000 loss: 0.14239746980369092\n",
      "  batch 46000 loss: 0.14380894646793604\n",
      "  batch 47000 loss: 0.1400110856667161\n",
      "  batch 48000 loss: 0.1399241019114852\n",
      "  batch 49000 loss: 0.14180239955708385\n",
      "  batch 50000 loss: 0.14195550356805325\n",
      "  batch 51000 loss: 0.1448727139532566\n",
      "  batch 52000 loss: 0.1411844965480268\n",
      "  batch 53000 loss: 0.14246396435052155\n",
      "  batch 54000 loss: 0.1401705573797226\n",
      "  batch 55000 loss: 0.14875720803439618\n",
      "  batch 56000 loss: 0.14000671649724244\n",
      "  batch 57000 loss: 0.14258969021588563\n",
      "  batch 58000 loss: 0.14562222607433795\n",
      "  batch 59000 loss: 0.14098973645269872\n",
      "  batch 60000 loss: 0.1399424448981881\n",
      "  batch 61000 loss: 0.1384100178256631\n",
      "  batch 62000 loss: 0.1424313070476055\n",
      "  batch 63000 loss: 0.14001367213577032\n",
      "  batch 64000 loss: 0.1425549301728606\n",
      "  batch 65000 loss: 0.13598141609877348\n",
      "  batch 66000 loss: 0.13994664598256348\n",
      "  batch 67000 loss: 0.13923970517516135\n",
      "  batch 68000 loss: 0.13878750687837602\n",
      "  batch 69000 loss: 0.13944248414039612\n",
      "  batch 70000 loss: 0.1417373463883996\n",
      "  batch 71000 loss: 0.14000214369595052\n",
      "  batch 72000 loss: 0.141975678935647\n",
      "  batch 73000 loss: 0.1370090100467205\n",
      "  batch 74000 loss: 0.14223112580925226\n",
      "  batch 75000 loss: 0.13823339704424142\n",
      "  batch 76000 loss: 0.13773248276114464\n",
      "  batch 77000 loss: 0.13801049426943063\n",
      "  batch 78000 loss: 0.13705067988485098\n",
      "  batch 79000 loss: 0.1372402559891343\n",
      "  batch 80000 loss: 0.13836937484145165\n",
      "  batch 81000 loss: 0.13699557959288358\n",
      "  batch 82000 loss: 0.14130442180484534\n",
      "  batch 83000 loss: 0.14193429347127676\n",
      "  batch 84000 loss: 0.13940309401601553\n",
      "  batch 85000 loss: 0.1435803639218211\n",
      "  batch 86000 loss: 0.13763495198637246\n",
      "  batch 87000 loss: 0.13875219224393368\n",
      "  batch 88000 loss: 0.14164665733277798\n",
      "  batch 89000 loss: 0.1407604601085186\n",
      "  batch 90000 loss: 0.1361563702970743\n",
      "  batch 91000 loss: 0.14124381514638662\n",
      "  batch 92000 loss: 0.13555582232028246\n",
      "  batch 93000 loss: 0.13969877285510302\n",
      "  batch 94000 loss: 0.13610104227811098\n",
      "  batch 95000 loss: 0.1365946488082409\n",
      "  batch 96000 loss: 0.13847367649525405\n",
      "  batch 97000 loss: 0.13956825547665358\n",
      "  batch 98000 loss: 0.13905991216003893\n",
      "  batch 99000 loss: 0.1377192604392767\n",
      "  batch 100000 loss: 0.14122727885097267\n",
      "  batch 101000 loss: 0.13789339685440063\n",
      "  batch 102000 loss: 0.13694785705208778\n",
      "  batch 103000 loss: 0.13382010378688575\n",
      "  batch 104000 loss: 0.1367241027355194\n",
      "  batch 105000 loss: 0.13361266461759805\n",
      "  batch 106000 loss: 0.13859755635261536\n",
      "  batch 107000 loss: 0.13626131227612495\n",
      "  batch 108000 loss: 0.13655574755370617\n",
      "  batch 109000 loss: 0.13312432474642993\n",
      "  batch 110000 loss: 0.13830578061193227\n",
      "  batch 111000 loss: 0.13390419835597275\n",
      "  batch 112000 loss: 0.14040195462107657\n",
      "  batch 113000 loss: 0.13685355820506812\n",
      "  batch 114000 loss: 0.13791623537242412\n",
      "  batch 115000 loss: 0.13622533390671016\n",
      "  batch 116000 loss: 0.1336862862110138\n",
      "  batch 117000 loss: 0.1382592311128974\n",
      "LOSS train 0.1382592311128974 valid 0.12656190991401672\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.13866759230196476\n",
      "  batch 2000 loss: 0.13744331349432468\n",
      "  batch 3000 loss: 0.13621028713136912\n",
      "  batch 4000 loss: 0.1359555240496993\n",
      "  batch 5000 loss: 0.1394972269460559\n",
      "  batch 6000 loss: 0.138683664791286\n",
      "  batch 7000 loss: 0.13496982584148645\n",
      "  batch 8000 loss: 0.13620225891470908\n",
      "  batch 9000 loss: 0.1361818661019206\n",
      "  batch 10000 loss: 0.13610002486407757\n",
      "  batch 11000 loss: 0.1374354292228818\n",
      "  batch 12000 loss: 0.13832739106565714\n",
      "  batch 13000 loss: 0.13288467141985894\n",
      "  batch 14000 loss: 0.13383197090029716\n",
      "  batch 15000 loss: 0.13515318590402603\n",
      "  batch 16000 loss: 0.13570199082046747\n",
      "  batch 17000 loss: 0.13766278578341007\n",
      "  batch 18000 loss: 0.13576660789176823\n",
      "  batch 19000 loss: 0.1374251070469618\n",
      "  batch 20000 loss: 0.13429027333855628\n",
      "  batch 21000 loss: 0.1295406268313527\n",
      "  batch 22000 loss: 0.13826198747754098\n",
      "  batch 23000 loss: 0.13408277106285096\n",
      "  batch 24000 loss: 0.13344092962518334\n",
      "  batch 25000 loss: 0.1333129023835063\n",
      "  batch 26000 loss: 0.13466232578083873\n",
      "  batch 27000 loss: 0.13282197761535644\n",
      "  batch 28000 loss: 0.1324458505436778\n",
      "  batch 29000 loss: 0.13163646845147015\n",
      "  batch 30000 loss: 0.13360038105398417\n",
      "  batch 31000 loss: 0.1315628240481019\n",
      "  batch 32000 loss: 0.13512716136872768\n",
      "  batch 33000 loss: 0.1339168816283345\n",
      "  batch 34000 loss: 0.13492117338627577\n",
      "  batch 35000 loss: 0.1335734189376235\n",
      "  batch 36000 loss: 0.1360603387951851\n",
      "  batch 37000 loss: 0.13470000328496098\n",
      "  batch 38000 loss: 0.1316941177472472\n",
      "  batch 39000 loss: 0.13238615097850562\n",
      "  batch 40000 loss: 0.13120219469815492\n",
      "  batch 41000 loss: 0.1340864058881998\n",
      "  batch 42000 loss: 0.13194394931942224\n",
      "  batch 43000 loss: 0.13059788011759518\n",
      "  batch 44000 loss: 0.13175081565231084\n",
      "  batch 45000 loss: 0.13197281866520644\n",
      "  batch 46000 loss: 0.13327763751894237\n",
      "  batch 47000 loss: 0.1334946901574731\n",
      "  batch 48000 loss: 0.1298343309313059\n",
      "  batch 49000 loss: 0.13318062040954828\n",
      "  batch 50000 loss: 0.13226599561423064\n",
      "  batch 51000 loss: 0.13410709271579982\n",
      "  batch 52000 loss: 0.13188045414537192\n",
      "  batch 53000 loss: 0.13380290238559245\n",
      "  batch 54000 loss: 0.13123305744677782\n",
      "  batch 55000 loss: 0.13723989798873662\n",
      "  batch 56000 loss: 0.13325100257992745\n",
      "  batch 57000 loss: 0.13373020290583373\n",
      "  batch 58000 loss: 0.13262449015676975\n",
      "  batch 59000 loss: 0.13008206240832806\n",
      "  batch 60000 loss: 0.1305022044479847\n",
      "  batch 61000 loss: 0.12964675696194172\n",
      "  batch 62000 loss: 0.13343963288515806\n",
      "  batch 63000 loss: 0.13022407422959806\n",
      "  batch 64000 loss: 0.1325620998889208\n",
      "  batch 65000 loss: 0.13006779609620572\n",
      "  batch 66000 loss: 0.1340546585097909\n",
      "  batch 67000 loss: 0.12867248578742146\n",
      "  batch 68000 loss: 0.13429198710620402\n",
      "  batch 69000 loss: 0.13100109684467315\n",
      "  batch 70000 loss: 0.13417980487644673\n",
      "  batch 71000 loss: 0.13275244990736246\n",
      "  batch 72000 loss: 0.1360440843999386\n",
      "  batch 73000 loss: 0.12816278985142707\n",
      "  batch 74000 loss: 0.13227435697615147\n",
      "  batch 75000 loss: 0.13421870317310095\n",
      "  batch 76000 loss: 0.13123269912600516\n",
      "  batch 77000 loss: 0.1294820353835821\n",
      "  batch 78000 loss: 0.13022140455245973\n",
      "  batch 79000 loss: 0.13021732880920173\n",
      "  batch 80000 loss: 0.13134279750287534\n",
      "  batch 81000 loss: 0.13201086229085923\n",
      "  batch 82000 loss: 0.13417335142195225\n",
      "  batch 83000 loss: 0.13257111571729183\n",
      "  batch 84000 loss: 0.13120293371379377\n",
      "  batch 85000 loss: 0.13415266021341085\n",
      "  batch 86000 loss: 0.12982481411844493\n",
      "  batch 87000 loss: 0.13142628809064627\n",
      "  batch 88000 loss: 0.13325388901680707\n",
      "  batch 89000 loss: 0.13254463136196137\n",
      "  batch 90000 loss: 0.1289554210677743\n",
      "  batch 91000 loss: 0.13140274596959353\n",
      "  batch 92000 loss: 0.12790091648697854\n",
      "  batch 93000 loss: 0.13106253055483102\n",
      "  batch 94000 loss: 0.12851400727778672\n",
      "  batch 95000 loss: 0.12877188585698604\n",
      "  batch 96000 loss: 0.12928721697628498\n",
      "  batch 97000 loss: 0.1327286289334297\n",
      "  batch 98000 loss: 0.12979882556945085\n",
      "  batch 99000 loss: 0.1331419388949871\n",
      "  batch 100000 loss: 0.13031495061516762\n",
      "  batch 101000 loss: 0.1308398735895753\n",
      "  batch 102000 loss: 0.12800562620162964\n",
      "  batch 103000 loss: 0.13049080040305852\n",
      "  batch 104000 loss: 0.12623881954699756\n",
      "  batch 105000 loss: 0.1277850518003106\n",
      "  batch 106000 loss: 0.13135589884966611\n",
      "  batch 107000 loss: 0.1281348113194108\n",
      "  batch 108000 loss: 0.12719566202163696\n",
      "  batch 109000 loss: 0.1287780875340104\n",
      "  batch 110000 loss: 0.1299740892648697\n",
      "  batch 111000 loss: 0.12711233419924975\n",
      "  batch 112000 loss: 0.13025043810158968\n",
      "  batch 113000 loss: 0.13091052670776843\n",
      "  batch 114000 loss: 0.13227339413762093\n",
      "  batch 115000 loss: 0.13256696619838476\n",
      "  batch 116000 loss: 0.1281964287944138\n",
      "  batch 117000 loss: 0.12998411118239164\n",
      "LOSS train 0.12998411118239164 valid 0.11781615763902664\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.13105266399681567\n",
      "  batch 2000 loss: 0.1312165000140667\n",
      "  batch 3000 loss: 0.12807197800278664\n",
      "  batch 4000 loss: 0.12879158774763347\n",
      "  batch 5000 loss: 0.13075596178323032\n",
      "  batch 6000 loss: 0.13142514637857675\n",
      "  batch 7000 loss: 0.12875783351808787\n",
      "  batch 8000 loss: 0.12781224930286408\n",
      "  batch 9000 loss: 0.12793501258641482\n",
      "  batch 10000 loss: 0.12851697055995465\n",
      "  batch 11000 loss: 0.12901064658537506\n",
      "  batch 12000 loss: 0.1284725907742977\n",
      "  batch 13000 loss: 0.1276658913716674\n",
      "  batch 14000 loss: 0.1273746249563992\n",
      "  batch 15000 loss: 0.12997907011955975\n",
      "  batch 16000 loss: 0.12933991768956185\n",
      "  batch 17000 loss: 0.13113923528045415\n",
      "  batch 18000 loss: 0.1280442210510373\n",
      "  batch 19000 loss: 0.1319507290199399\n",
      "  batch 20000 loss: 0.1302770264521241\n",
      "  batch 21000 loss: 0.1286667590662837\n",
      "  batch 22000 loss: 0.1296859921850264\n",
      "  batch 23000 loss: 0.12827581232041121\n",
      "  batch 24000 loss: 0.12847777429223062\n",
      "  batch 25000 loss: 0.12581481942534448\n",
      "  batch 26000 loss: 0.12940196450799704\n",
      "  batch 27000 loss: 0.12847867708653213\n",
      "  batch 28000 loss: 0.1282580944597721\n",
      "  batch 29000 loss: 0.12576159916073085\n",
      "  batch 30000 loss: 0.1294464386999607\n",
      "  batch 31000 loss: 0.1260427795276046\n",
      "  batch 32000 loss: 0.1300943739786744\n",
      "  batch 33000 loss: 0.12953934809938072\n",
      "  batch 34000 loss: 0.1281096126139164\n",
      "  batch 35000 loss: 0.1306999681890011\n",
      "  batch 36000 loss: 0.127399346858263\n",
      "  batch 37000 loss: 0.1281083502471447\n",
      "  batch 38000 loss: 0.12495676148682833\n",
      "  batch 39000 loss: 0.12689663879945876\n",
      "  batch 40000 loss: 0.12565470433980228\n",
      "  batch 41000 loss: 0.12563774532079697\n",
      "  batch 42000 loss: 0.12575028611719608\n",
      "  batch 43000 loss: 0.12564107619225978\n",
      "  batch 44000 loss: 0.12555603175610303\n",
      "  batch 45000 loss: 0.12765101718902588\n",
      "  batch 46000 loss: 0.1311154256761074\n",
      "  batch 47000 loss: 0.1269633003845811\n",
      "  batch 48000 loss: 0.1235297082439065\n",
      "  batch 49000 loss: 0.1261107037514448\n",
      "  batch 50000 loss: 0.12856190678477286\n",
      "  batch 51000 loss: 0.1277993116825819\n",
      "  batch 52000 loss: 0.12845314398407937\n",
      "  batch 53000 loss: 0.13078040789812803\n",
      "  batch 54000 loss: 0.12677854286134244\n",
      "  batch 55000 loss: 0.12719974948465823\n",
      "  batch 56000 loss: 0.1272441842406988\n",
      "  batch 57000 loss: 0.12609614016115667\n",
      "  batch 58000 loss: 0.12804282626509667\n",
      "  batch 59000 loss: 0.12660896737873553\n",
      "  batch 60000 loss: 0.12432472269982099\n",
      "  batch 61000 loss: 0.12436773376911879\n",
      "  batch 62000 loss: 0.12626831675320863\n",
      "  batch 63000 loss: 0.1262339630872011\n",
      "  batch 64000 loss: 0.1275553461536765\n",
      "  batch 65000 loss: 0.12547668523341418\n",
      "  batch 66000 loss: 0.12590149448812007\n",
      "  batch 67000 loss: 0.12378691101074218\n",
      "  batch 68000 loss: 0.12772071803361176\n",
      "  batch 69000 loss: 0.12319497064501048\n",
      "  batch 70000 loss: 0.12935123653709887\n",
      "  batch 71000 loss: 0.12647507500648497\n",
      "  batch 72000 loss: 0.12911865746974946\n",
      "  batch 73000 loss: 0.1234985858425498\n",
      "  batch 74000 loss: 0.12768853595107793\n",
      "  batch 75000 loss: 0.12946727754175663\n",
      "  batch 76000 loss: 0.1263815091624856\n",
      "  batch 77000 loss: 0.12238306757807732\n",
      "  batch 78000 loss: 0.1250621172040701\n",
      "  batch 79000 loss: 0.12419419505447149\n",
      "  batch 80000 loss: 0.1257916785031557\n",
      "  batch 81000 loss: 0.12712036716192962\n",
      "  batch 82000 loss: 0.1288855295404792\n",
      "  batch 83000 loss: 0.12717286878824233\n",
      "  batch 84000 loss: 0.1248792845532298\n",
      "  batch 85000 loss: 0.13069014709442853\n",
      "  batch 86000 loss: 0.12806637838482857\n",
      "  batch 87000 loss: 0.12704280275851487\n",
      "  batch 88000 loss: 0.1290898354500532\n",
      "  batch 89000 loss: 0.12876349142193794\n",
      "  batch 90000 loss: 0.12634498390555382\n",
      "  batch 91000 loss: 0.12631077606976032\n",
      "  batch 92000 loss: 0.12250757198035717\n",
      "  batch 93000 loss: 0.12722495061904193\n",
      "  batch 94000 loss: 0.12166690977290273\n",
      "  batch 95000 loss: 0.12435370780527592\n",
      "  batch 96000 loss: 0.12487572064250707\n",
      "  batch 97000 loss: 0.1263291178792715\n",
      "  batch 98000 loss: 0.12383956911414862\n",
      "  batch 99000 loss: 0.1281090607047081\n",
      "  batch 100000 loss: 0.12639846646785735\n",
      "  batch 101000 loss: 0.1262743464410305\n",
      "  batch 102000 loss: 0.1224511701837182\n",
      "  batch 103000 loss: 0.12548483794927598\n",
      "  batch 104000 loss: 0.12418723369389773\n",
      "  batch 105000 loss: 0.12514727123081684\n",
      "  batch 106000 loss: 0.12614803733676672\n",
      "  batch 107000 loss: 0.12323116085678339\n",
      "  batch 108000 loss: 0.1246509321630001\n",
      "  batch 109000 loss: 0.12480281001329421\n",
      "  batch 110000 loss: 0.12336358667165041\n",
      "  batch 111000 loss: 0.125020602427423\n",
      "  batch 112000 loss: 0.1269119131118059\n",
      "  batch 113000 loss: 0.12518485357612372\n",
      "  batch 114000 loss: 0.12690107757598162\n",
      "  batch 115000 loss: 0.12611685142666101\n",
      "  batch 116000 loss: 0.12353561116382479\n",
      "  batch 117000 loss: 0.12718577660620212\n",
      "LOSS train 0.12718577660620212 valid 0.12359946966171265\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.12771943178027867\n",
      "  batch 2000 loss: 0.12704527469724416\n",
      "  batch 3000 loss: 0.12578314543515443\n",
      "  batch 4000 loss: 0.12619082368910312\n",
      "  batch 5000 loss: 0.1273902190029621\n",
      "  batch 6000 loss: 0.1250594160184264\n",
      "  batch 7000 loss: 0.12585480127483606\n",
      "  batch 8000 loss: 0.12344719014316798\n",
      "  batch 9000 loss: 0.12454765919595957\n",
      "  batch 10000 loss: 0.1240187044814229\n",
      "  batch 11000 loss: 0.12473232087492943\n",
      "  batch 12000 loss: 0.1276232166290283\n",
      "  batch 13000 loss: 0.12407917149364948\n",
      "  batch 14000 loss: 0.12525502725690604\n",
      "  batch 15000 loss: 0.12904876086860895\n",
      "  batch 16000 loss: 0.12490028823912144\n",
      "  batch 17000 loss: 0.12488850304484367\n",
      "  batch 18000 loss: 0.12348118983954191\n",
      "  batch 19000 loss: 0.12602010961622\n",
      "  batch 20000 loss: 0.12353033772855997\n",
      "  batch 21000 loss: 0.12080356492102146\n",
      "  batch 22000 loss: 0.1280527152083814\n",
      "  batch 23000 loss: 0.12568268225342036\n",
      "  batch 24000 loss: 0.12494429580494762\n",
      "  batch 25000 loss: 0.12346865464746952\n",
      "  batch 26000 loss: 0.12556058735027908\n",
      "  batch 27000 loss: 0.12635762142390014\n",
      "  batch 28000 loss: 0.12400793650746346\n",
      "  batch 29000 loss: 0.12239259546995163\n",
      "  batch 30000 loss: 0.12290853551030159\n",
      "  batch 31000 loss: 0.12157832594960928\n",
      "  batch 32000 loss: 0.12631483433395624\n",
      "  batch 33000 loss: 0.1267494507022202\n",
      "  batch 34000 loss: 0.12447274097800255\n",
      "  batch 35000 loss: 0.12652518140524627\n",
      "  batch 36000 loss: 0.12328442353755235\n",
      "  batch 37000 loss: 0.12734031607210636\n",
      "  batch 38000 loss: 0.1230018006041646\n",
      "  batch 39000 loss: 0.1230348605439067\n",
      "  batch 40000 loss: 0.12253804600983859\n",
      "  batch 41000 loss: 0.12288473331928253\n",
      "  batch 42000 loss: 0.12270656172931195\n",
      "  batch 43000 loss: 0.12286753360927105\n",
      "  batch 44000 loss: 0.12469945397228002\n",
      "  batch 45000 loss: 0.12408058018609881\n",
      "  batch 46000 loss: 0.12570805267244578\n",
      "  batch 47000 loss: 0.12364055989682675\n",
      "  batch 48000 loss: 0.12024132692068815\n",
      "  batch 49000 loss: 0.12317973728477954\n",
      "  batch 50000 loss: 0.12338494455814361\n",
      "  batch 51000 loss: 0.12551356711238623\n",
      "  batch 52000 loss: 0.12542999236285687\n",
      "  batch 53000 loss: 0.1273927412033081\n",
      "  batch 54000 loss: 0.12400485183298587\n",
      "  batch 55000 loss: 0.12364704461395741\n",
      "  batch 56000 loss: 0.12574626169353723\n",
      "  batch 57000 loss: 0.1239551664069295\n",
      "  batch 58000 loss: 0.12616807150095702\n",
      "  batch 59000 loss: 0.12421960056573153\n",
      "  batch 60000 loss: 0.12262324577569962\n",
      "  batch 61000 loss: 0.121221973977983\n",
      "  batch 62000 loss: 0.12220174045115709\n",
      "  batch 63000 loss: 0.12410385286062955\n",
      "  batch 64000 loss: 0.12402369098365307\n",
      "  batch 65000 loss: 0.12148087741434574\n",
      "  batch 66000 loss: 0.12295404603332281\n",
      "  batch 67000 loss: 0.12240321995317936\n",
      "  batch 68000 loss: 0.12565347430855037\n",
      "  batch 69000 loss: 0.12156878419220447\n",
      "  batch 70000 loss: 0.12673178593069315\n",
      "  batch 71000 loss: 0.1245524790212512\n",
      "  batch 72000 loss: 0.1260869658216834\n",
      "  batch 73000 loss: 0.12194990500435234\n",
      "  batch 74000 loss: 0.12572164811939002\n",
      "  batch 75000 loss: 0.12496400913968682\n",
      "  batch 76000 loss: 0.12336953768879175\n",
      "  batch 77000 loss: 0.12275306785851717\n",
      "  batch 78000 loss: 0.12115154193341732\n",
      "  batch 79000 loss: 0.12194033417850733\n",
      "  batch 80000 loss: 0.12320619253441692\n",
      "  batch 81000 loss: 0.12516509564965964\n",
      "  batch 82000 loss: 0.12564306178689003\n",
      "  batch 83000 loss: 0.12423019096255303\n",
      "  batch 84000 loss: 0.12238366782665253\n",
      "  batch 85000 loss: 0.12683567252755165\n",
      "  batch 86000 loss: 0.12453148587048053\n",
      "  batch 87000 loss: 0.12388304571062327\n",
      "  batch 88000 loss: 0.12445945695787668\n",
      "  batch 89000 loss: 0.1252389813065529\n",
      "  batch 90000 loss: 0.12402496199309826\n",
      "  batch 91000 loss: 0.12492072468250989\n",
      "  batch 92000 loss: 0.12193359906971454\n",
      "  batch 93000 loss: 0.12329759980738163\n",
      "  batch 94000 loss: 0.11891668669134378\n",
      "  batch 95000 loss: 0.12143932604044676\n",
      "  batch 96000 loss: 0.12217119125276804\n",
      "  batch 97000 loss: 0.12422681510448456\n",
      "  batch 98000 loss: 0.12268848396092653\n",
      "  batch 99000 loss: 0.1239423174187541\n",
      "  batch 100000 loss: 0.12352566313743592\n",
      "  batch 101000 loss: 0.12326535297185183\n",
      "  batch 102000 loss: 0.12152490139007568\n",
      "  batch 103000 loss: 0.12143462552875281\n",
      "  batch 104000 loss: 0.12101202713698149\n",
      "  batch 105000 loss: 0.12120265028625726\n",
      "  batch 106000 loss: 0.12542877326905727\n",
      "  batch 107000 loss: 0.12368830381333829\n",
      "  batch 108000 loss: 0.12242101893573999\n",
      "  batch 109000 loss: 0.12211545649915934\n",
      "  batch 110000 loss: 0.12051845097541809\n",
      "  batch 111000 loss: 0.12159130105376244\n",
      "  batch 112000 loss: 0.12579936696216465\n",
      "  batch 113000 loss: 0.12241116251051426\n",
      "  batch 114000 loss: 0.12499863684177398\n",
      "  batch 115000 loss: 0.12394753943383693\n",
      "  batch 116000 loss: 0.12033796582370997\n",
      "  batch 117000 loss: 0.12246110093593597\n",
      "LOSS train 0.12246110093593597 valid 0.11121563613414764\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.12476100538671017\n",
      "  batch 2000 loss: 0.12367546704411507\n",
      "  batch 3000 loss: 0.12103647878766059\n",
      "  batch 4000 loss: 0.12383080366253853\n",
      "  batch 5000 loss: 0.1251962009370327\n",
      "  batch 6000 loss: 0.12401064562052488\n",
      "  batch 7000 loss: 0.1250213919878006\n",
      "  batch 8000 loss: 0.12173089238256216\n",
      "  batch 9000 loss: 0.12383603715151549\n",
      "  batch 10000 loss: 0.1213094784244895\n",
      "  batch 11000 loss: 0.12450204327702523\n",
      "  batch 12000 loss: 0.12388854156434535\n",
      "  batch 13000 loss: 0.12133396323025226\n",
      "  batch 14000 loss: 0.12224749514460563\n",
      "  batch 15000 loss: 0.12395143982768059\n",
      "  batch 16000 loss: 0.1241266590282321\n",
      "  batch 17000 loss: 0.1227669813260436\n",
      "  batch 18000 loss: 0.12180688478797674\n",
      "  batch 19000 loss: 0.1238584461286664\n",
      "  batch 20000 loss: 0.1215212582424283\n",
      "  batch 21000 loss: 0.12141743684560061\n",
      "  batch 22000 loss: 0.12549178542941808\n",
      "  batch 23000 loss: 0.12385744844004512\n",
      "  batch 24000 loss: 0.12276238878071308\n",
      "  batch 25000 loss: 0.1224691970422864\n",
      "  batch 26000 loss: 0.12120870194584131\n",
      "  batch 27000 loss: 0.12306509198993444\n",
      "  batch 28000 loss: 0.12209821120649576\n",
      "  batch 29000 loss: 0.11965580319613218\n",
      "  batch 30000 loss: 0.12207980062067508\n",
      "  batch 31000 loss: 0.11937145607173442\n",
      "  batch 32000 loss: 0.124473867893219\n",
      "  batch 33000 loss: 0.12602366338670254\n",
      "  batch 34000 loss: 0.12160039112716914\n",
      "  batch 35000 loss: 0.12311524190008641\n",
      "  batch 36000 loss: 0.12076472297683358\n",
      "  batch 37000 loss: 0.1227851627394557\n",
      "  batch 38000 loss: 0.12169200179725885\n",
      "  batch 39000 loss: 0.1192431040480733\n",
      "  batch 40000 loss: 0.12028885584324599\n",
      "  batch 41000 loss: 0.11973512791097164\n",
      "  batch 42000 loss: 0.12051180998235941\n",
      "  batch 43000 loss: 0.1215366118401289\n",
      "  batch 44000 loss: 0.1220133665278554\n",
      "  batch 45000 loss: 0.12242056142911314\n",
      "  batch 46000 loss: 0.12312724812328815\n",
      "  batch 47000 loss: 0.11975733686983585\n",
      "  batch 48000 loss: 0.1203047276288271\n",
      "  batch 49000 loss: 0.12091635627299548\n",
      "  batch 50000 loss: 0.12157474711164833\n",
      "  batch 51000 loss: 0.12329129533469677\n",
      "  batch 52000 loss: 0.12203319524228573\n",
      "  batch 53000 loss: 0.12655850491672754\n",
      "  batch 54000 loss: 0.12076695813983679\n",
      "  batch 55000 loss: 0.1208849052041769\n",
      "  batch 56000 loss: 0.12195171665400267\n",
      "  batch 57000 loss: 0.12058726695179939\n",
      "  batch 58000 loss: 0.12309476540237665\n",
      "  batch 59000 loss: 0.12305684402212501\n",
      "  batch 60000 loss: 0.11924947358295321\n",
      "  batch 61000 loss: 0.1184150353372097\n",
      "  batch 62000 loss: 0.12207789200544357\n",
      "  batch 63000 loss: 0.12157316706329584\n",
      "  batch 64000 loss: 0.12196721817553044\n",
      "  batch 65000 loss: 0.12002207551896572\n",
      "  batch 66000 loss: 0.12130196906626224\n",
      "  batch 67000 loss: 0.1199296870008111\n",
      "  batch 68000 loss: 0.12242477113008499\n",
      "  batch 69000 loss: 0.11934179001301527\n",
      "  batch 70000 loss: 0.1236434131115675\n",
      "  batch 71000 loss: 0.12069097451120615\n",
      "  batch 72000 loss: 0.12393294033408166\n",
      "  batch 73000 loss: 0.1211769188567996\n",
      "  batch 74000 loss: 0.1227324836552143\n",
      "  batch 75000 loss: 0.12259529073536396\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model_lstm.train(True)\n",
    "    avg_loss = train_one_epoch_lstm(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model_lstm.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader_rnn):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model_lstm(vinputs)\n",
    "        vlabels = torch.unsqueeze(vlabels, 1) # added to ensure labels and outputs are of the same size\n",
    "        vloss = loss_fn_lstm(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path_lstm = 'model_lstm_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model_lstm.state_dict(), model_path_lstm)\n",
    "\n",
    "    epoch_number += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11a39a0d09cc5009ec4bb720bab51ac91bcdef19676420c2ad1822c8de114940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
